{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tf2gpt.model import GPT\n",
    "from utils.story_util import Story,Stories\n",
    "from utils.progress_bar import ProgressBar\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16 <dtype: 'float16'> True\n"
     ]
    }
   ],
   "source": [
    "#with mirrored_strategy.scope():\n",
    "gpt = GPT(\n",
    "    vocab_size=30_000,\n",
    "    layer_size=32,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=2560,\n",
    "    num_attention_heads=32,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0,\n",
    "    train_size=499\n",
    ")\n",
    "gpt.load_weights('./gpt_weight_pretrain/weight_fp16')\n",
    "\n",
    "#input_x = tf.keras.layers.Input((499,), dtype=tf.int32)\n",
    "#outputs = gpt_origin(input_x)\n",
    "\n",
    "#gpt = tf.keras.Model(inputs=input_x, outputs=outputs)\n",
    "#gpt = multi_gpu_model(gpt, gpus=8)\n",
    "\n",
    "print(tf.keras.backend.floatx(), tf.float16, tf.keras.backend.floatx() == tf.float16)\n",
    "if tf.keras.backend.floatx() == tf.float16:\n",
    "    for x in gpt.weights:\n",
    "        assert x.dtype == tf.float16\n",
    "\n",
    "\n",
    "gpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gpt2_tokenizer import GPT2Tokenizer\n",
    "cbpe = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.592 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[837, 259, 497, 57, 8, 237]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气还行')\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "气 不错 不错行 \n",
      "------------------------------\n",
      "气 不错 不错行 ,\n",
      "------------------------------\n",
      "气 不错 不错行 , 我\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着 去\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着 去 看看\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着 去 看看 \n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着 去 看看 ,\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_basic_logic():\n",
    "    ids = cbpe.encode('今天天气还行')\n",
    "    #print(ids)\n",
    "    print(\"+\" * 20)\n",
    "    for i in range(10):\n",
    "        output = gpt(tf.constant([ids]))\n",
    "        #print(output[0].shape)\n",
    "        nid = np.argmax(output[0, -1])\n",
    "        ids += [nid]\n",
    "        #print(i, cbpe.decode(ids))\n",
    "        #print(np.argmax(output[0],axis=-1))\n",
    "        print(cbpe.decode(np.argmax(output[0],axis=-1)))\n",
    "        print('-' * 30)\n",
    "test_basic_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(learning_rate=6e-4,\n",
    "                      warmup_steps=20_0000,\n",
    "                      decay_steps=200_0000,\n",
    "                      alpha=0.0):\n",
    "    def decayed_learning_rate(step=1):\n",
    "        if step <= warmup_steps:\n",
    "            mult = step / float(warmup_steps)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (decay_steps - warmup_steps)\n",
    "            mult = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "            mult = max(0.1, mult)\n",
    "        return learning_rate * mult\n",
    "    return decayed_learning_rate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#stories = Stories(\"./labeled_data/advanture_translated/processed_translated_story.txt\").stories\n",
    "stories = Stories(\"./labeled_data/advanture_translated/processed_translated_story.txt\").stories\n",
    "#stories = stories[:50]\n",
    "data_folder = \"./labeled_data/\"\n",
    "txt_files = [(data_folder + i) for i in os.listdir(data_folder) if \"txt\" in i]\n",
    "#stories = stories[:10]\n",
    "stories = [Story(\"\",\"\").from_file(i) for i in txt_files]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "valid_stories = Stories(\"./labeled_data/advanture_translated/processed_translated_story_valid.txt\").stories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(stories),len(valid_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recoard = json.load(open('./labeled_data/advanture_translated/truncated_advanture_train.json'))\n",
    "valid_recoard = json.load(open('./labeled_data/advanture_translated/truncated_advanture_valid.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 266)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_recoard),len(valid_recoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def data_generator(stories, batch_size=4,sample_len=200,inf=False):\n",
    "    while True:\n",
    "        batch_data = []\n",
    "        tmp_stories = copy.copy(stories)\n",
    "        random.shuffle(tmp_stories)\n",
    "        for i,one_story in enumerate(tmp_stories):\n",
    "            story_content = one_story.to_dungeon_format()\n",
    "            story_content = story_content.replace(\"<start>\\n\",\"\")\n",
    "            story_content = story_content.replace(\"\\n<end>\",\"\")\n",
    "            story_content = story_content.replace(\"\\n<end>\",\"\")\n",
    "            story_content = story_content.replace(\" \",\"\")\n",
    "            ids = cbpe.encode(story_content)\n",
    "            while ids:\n",
    "                sample = ids[:sample_len]\n",
    "                ids = ids[sample_len:]\n",
    "                if len(sample) < sample_len:\n",
    "                    sample += [0 for i in range((sample_len - len(sample)))]\n",
    "                batch_data.append(sample)\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    yield np.asarray(batch_data)\n",
    "                    batch_data = []\n",
    "        if not inf:\n",
    "            break\n",
    "            \n",
    "import copy\n",
    "def data_generator_content(texts, batch_size=4,sample_len=200,inf=False):\n",
    "    tmp_text = copy.copy(texts)\n",
    "    segments = []\n",
    "    while tmp_text:\n",
    "        story_content = tmp_text[:10000]\n",
    "        segments.append(story_content)\n",
    "        tmp_text = tmp_text[10000:]\n",
    "    while True:\n",
    "        \n",
    "        batch_data = []\n",
    "        random.shuffle(segments)\n",
    "        for one_story_context in segments:\n",
    "            ids = cbpe.encode(one_story_context)\n",
    "            while ids:\n",
    "                sample = ids[:sample_len]\n",
    "                ids = ids[sample_len:]\n",
    "                if len(sample) < sample_len:\n",
    "                    sample += [0 for i in range((sample_len - len(sample)))]\n",
    "                batch_data.append(sample)\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    yield np.asarray(batch_data)\n",
    "                    batch_data = []\n",
    "        if not inf:\n",
    "            break\n",
    "            \n",
    "def turb_sentence(inputs):\n",
    "    outputs = []\n",
    "    for i in inputs:\n",
    "        if random.random() > 0.1:\n",
    "            outputs.append(i)\n",
    "    return outputs\n",
    "            \n",
    "def data_generator_json(stories, batch_size=1,inf=False):\n",
    "    while True:\n",
    "        tmp_stories = copy.copy(stories)\n",
    "        random.shuffle(tmp_stories)\n",
    "        for i,one_story in enumerate(tmp_stories):\n",
    "            previous_contents = one_story[\"previous\"]\n",
    "            previous_contents = turb_sentence(previous_contents)\n",
    "            action = one_story[\"action\"]\n",
    "            content = one_story[\"result\"]\n",
    "            \n",
    "            previous_contents = ''.join(previous_contents)\n",
    "            content = ''.join(content)\n",
    "            \n",
    "            ids_prev = cbpe.encode(previous_contents + \"\\n\")\n",
    "            ids_action = cbpe.encode(\">\" + action + \"\\n\")\n",
    "            ids_content = cbpe.encode(content)[:300]\n",
    "            \n",
    "            ids_prev = ids_prev + ids_action\n",
    "            \n",
    "            prev_context_len = int(random.uniform(100,500))\n",
    "            ids_prev = ids_prev[-prev_context_len:]\n",
    "            \n",
    "            overall_ids = ids_prev + ids_content\n",
    "            mask = [0 for i in ids_prev] + [1 for i in ids_content]\n",
    "            yield np.asarray([overall_ids]),np.asarray([mask])\n",
    "            \n",
    "        if not inf:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = data_generator_json(valid_recoard,inf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,m = valid_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 687), (1, 687))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"。 当 里德 看到 他 的 妻子 玛格丽特 和 他们 的 几个 孩子 时 , 他 高兴 地 叫 了 一声 , 冲 上前 拥抱 他们 。 你 很快 就 了解 到 一些 非常 令人担忧 的 事情 : 在 山上 的 营地 , 唐纳 党 的 其余 成员 正因 饥饿 而 不断 死亡 , 刚 从 山地车 司机 约翰 丹顿 和 孩子 艾达凯 塞 伯格 下山 , 在 第一次 救济 旅行 时 死亡 .   19   名 幸存者 留在 了 第一次 救援 , 但 更 多 的 人 仍然 在 山上 , 包括 里德 的 两个 孩子 。 威廉 · 麦卡琴 得知 他 年幼 的 女儿 哈丽特 在 山上 去世 了 , 他 很 难 接受 这个 消息 。 当 您 在 冰雪 覆盖 的 荒原 上 月 的 第一天 到达 客舱 的 乱堆 的 第一 人 , 你 相遇 是 里德 “ 的 女儿 帕蒂 , 对 她 的 父亲 , 他们 用 欢呼 迎接 冲刺 。 其他 一些 人影 从小 屋里 出来 , 包括 帕特里克 · 布林 和 路易斯 · 凯塞 伯格 , 总共 有   19   人 仍然 住 在 特拉 基湖 周围 的 小 屋里 , 大约 有   12   人住 在 奥尔德 溪 , 包括 乔治 唐纳 。 只 呆 一天 , 你 就 聚集 了 十七名 幸存者 , 你 可以 把 所有 的 人 都 带走 。 进行 了 一些 讨论 , 并 同意 留下 一两个 人来 帮助 留下来 的   13   个人 , 尤其 是 在   Alder   Creek   的 那些 人 。 你 做 什么 工作 ? \\n > 你 带 着 第二次 救济 的 其余部分 离开 \\n 你 选择 陪 在 第二 浮雕 和 17 个 定居 的 艰难 跋涉 回到 萨特 ' 堡垒 。 当 一场 风暴 将 你们 所有人 困在 开阔 的 地面 上 , 艾萨克 · 唐纳   ( Isaac   Donner )   去世 时 , 事情 的 开端 并 不 顺利 。 James   Reed   和   William   McCutchen   决定 , 因为 供应量 太 少 , 他们 必须 继续前进 , 但 大多数 定居者 太 虚弱 而 无法 移动 。 带 着 里德 的 两个 孩子 和 第三个 孩子 继续前进 , 无法 和 其他人 呆 在 一起 。 几天 后 , 您 遇到 了 带领 救援队 上山 的 威廉 · 艾迪 和威廉 · 福斯特 。 里德 引导 他们 到 你 留下 的 幸存者 在 那里 和 你 做 回 约翰逊 三月 中旬 “ 小号 牧场 。 你 离开 的 定居者 中有   11   人 在 两天 后 抵达 , 其中 两人 已经 死亡 。 艾迪 和 福斯特 带 着 另外 五名 幸存者 返回 后 不久 , 报告 说 不 可能 再有 幸存者 了 。 George   Donner 、 他 的 妻子   Tamzene   和 侄子   Samuel 、 Louis   Keseberg   和\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbpe.decode(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "tl: 2.453125 vl: 2.912109375 35.07 % [=================>---------------------------------] 1578/4500 \t used:1748s eta:3237 s9 s s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN or Inf found in input tensor.\n",
      "tl: 2.486328125 vl: 3.01171875 100.00 % [==================================================>] 4500/4500 \t used:4933s eta:0 sssss17 s\n",
      "Epoch 2\n",
      "tl: 2.33984375 vl: 2.556640625 97.20 % [================================================>--] 4374/4500 \t used:4759s eta:137 sssss8 sWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.974609375 vl: 2.814453125 100.00 % [==================================================>] 4500/4500 \t used:4897s eta:0 ss\n",
      "Epoch 3\n",
      "tl: 2.671875 vl: 1.8720703125 44.38 % [======================>----------------------------] 1997/4500 \t used:2171s eta:2721 s sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.12109375 vl: 2.6640625 100.00 % [==================================================>] 4500/4500 \t used:4893s eta:0 s s ssss\n",
      "Epoch 4\n",
      "tl: 3.0703125 vl: 2.626953125 9.42 % [====>----------------------------------------------] 424/4500 \t used:460s eta:4424 s ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.28515625 vl: 2.62890625 100.00 % [==================================================>] 4500/4500 \t used:4879s eta:0 sssssss9 s\n",
      "Epoch 5\n",
      "tl: 4.08203125 vl: 2.369140625 57.64 % [============================>----------------------] 2594/4500 \t used:2802s eta:2059 sss sWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.78515625 vl: 2.515625 100.00 % [==================================================>] 4500/4500 \t used:4868s eta:0 s1 ssss s\n",
      "Epoch 6\n",
      "tl: 2.6015625 vl: 2.287109375 75.18 % [=====================================>-------------] 3383/4500 \t used:3672s eta:1212 sss ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.8046875 vl: 3.2265625 100.00 % [==================================================>] 4500/4500 \t used:4886s eta:0 s:1 ssss\n",
      "Epoch 7\n",
      "tl: 2.587890625 vl: 2.978515625 73.64 % [====================================>--------------] 3314/4500 \t used:3583s eta:1282 sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.708984375 vl: 2.2265625 100.00 % [==================================================>] 4500/4500 \t used:4868s eta:0 s sss ss\n",
      "Epoch 8\n",
      "tl: 2.076171875 vl: 2.62890625 56.29 % [============================>----------------------] 2533/4500 \t used:2747s eta:2133 sss sWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.630859375 vl: 2.5078125 100.00 % [==================================================>] 4500/4500 \t used:4880s eta:0 ssssss s\n",
      "Epoch 9\n",
      "tl: 2.169921875 vl: 2.568359375 30.71 % [===============>-----------------------------------] 1382/4500 \t used:1501s eta:3387 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.32421875 vl: 3.64453125 100.00 % [==================================================>] 4500/4500 \t used:4890s eta:0 sssss s s\n",
      "Epoch 10\n",
      "tl: 2.380859375 vl: 2.185546875 96.18 % [================================================>--] 4328/4500 \t used:4694s eta:186 ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.328125 vl: 2.4296875 100.00 % [==================================================>] 4500/4500 \t used:4881s eta:0 s1 sss ss\n",
      "Epoch 11\n",
      "tl: 1.77734375 vl: 2.81640625 52.71 % [==========================>------------------------] 2372/4500 \t used:2573s eta:2308 sssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.36328125 vl: 2.01171875 100.00 % [==================================================>] 4500/4500 \t used:4881s eta:0 ssssssss\n",
      "Epoch 12\n",
      "tl: 2.5625 vl: 2.78515625 61.33 % [==============================>--------------------] 2760/4500 \t used:2997s eta:1889 ss ss ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.37109375 vl: 2.53125 100.00 % [==================================================>] 4500/4500 \t used:4886s eta:0 s1 sssssss\n",
      "Epoch 13\n",
      "tl: 2.55078125 vl: 2.708984375 72.22 % [====================================>--------------] 3250/4500 \t used:3527s eta:1356 ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.685546875 vl: 2.08203125 100.00 % [==================================================>] 4500/4500 \t used:4884s eta:0 ssss ss\n",
      "Epoch 14\n",
      "tl: 2.712890625 vl: 2.828125 11.51 % [=====>---------------------------------------------] 518/4500 \t used:562s eta:4324 ss ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.123046875 vl: 2.404296875 100.00 % [==================================================>] 4500/4500 \t used:4882s eta:0 sss s\n",
      "Epoch 15\n",
      "tl: 2.048828125 vl: 3.162109375 25.07 % [============>--------------------------------------] 1128/4500 \t used:1225s eta:3663 sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.439453125 vl: 3.109375 100.00 % [==================================================>] 4500/4500 \t used:4889s eta:0 sss ssss\n",
      "Epoch 16\n",
      "tl: 2.513671875 vl: 2.57421875 99.27 % [=================================================>-] 4467/4500 \t used:4843s eta:35 sssssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.80859375 vl: 3.248046875 100.00 % [==================================================>] 4500/4500 \t used:4879s eta:0 ss1 s\n",
      "Epoch 17\n",
      "tl: 2.669921875 vl: 3.267578125 25.84 % [============>--------------------------------------] 1163/4500 \t used:1263s eta:3623 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.40234375 vl: 3.427734375 100.00 % [==================================================>] 4500/4500 \t used:4889s eta:0 sssssss\n",
      "Epoch 18\n",
      "tl: 2.50390625 vl: 2.56640625 68.04 % [==================================>----------------] 3062/4500 \t used:3329s eta:1563 ssss sWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.748046875 vl: 2.744140625 100.00 % [==================================================>] 4500/4500 \t used:4890s eta:0 ss s\n",
      "Epoch 19\n",
      "tl: 2.23046875 vl: 2.53125 46.20 % [=======================>---------------------------] 2079/4500 \t used:2259s eta:2631 ssssssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.83203125 vl: 3.509765625 100.00 % [==================================================>] 4500/4500 \t used:4886s eta:0 sssss ss\n",
      "Epoch 20\n",
      "tl: 2.67578125 vl: 2.171875 58.73 % [=============================>---------------------] 2643/4500 \t used:2867s eta:2014 ss ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 3.509765625 vl: 2.447265625 100.00 % [==================================================>] 4500/4500 \t used:4879s eta:0 ssssss\n",
      "Epoch 21\n",
      "tl: 2.369140625 vl: 2.05078125 17.20 % [========>------------------------------------------] 774/4500 \t used:840s eta:4046 sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.837890625 vl: 2.966796875 100.00 % [==================================================>] 4500/4500 \t used:4883s eta:0 sssss s\n",
      "Epoch 22\n",
      "tl: 2.578125 vl: 2.982421875 16.13 % [========>------------------------------------------] 726/4500 \t used:787s eta:4093 s4 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.98828125 vl: 3.2265625 100.00 % [==================================================>] 4500/4500 \t used:4878s eta:0 ss ssss ss\n",
      "Epoch 23\n",
      "tl: 2.390625 vl: 3.5078125 7.38 % [===>-----------------------------------------------] 332/4500 \t used:359s eta:4506 s7 ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.60546875 vl: 2.4609375 100.00 % [==================================================>] 4500/4500 \t used:4877s eta:0 sssss5 s\n",
      "Epoch 24\n",
      "tl: 2.53515625 vl: 2.580078125 91.56 % [=============================================>-----] 4120/4500 \t used:4471s eta:412 sssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 1.552734375 vl: 2.865234375 100.00 % [==================================================>] 4500/4500 \t used:4883s eta:0 sss\n",
      "Epoch 25\n",
      "tl: 2.28515625 vl: 2.416015625 54.40 % [===========================>-----------------------] 2448/4500 \t used:2655s eta:2225 ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.625 vl: 2.734375 100.00 % [==================================================>] 4500/4500 \t used:4884s eta:0 s s sss sssss s\n",
      "Epoch 26\n",
      "tl: 2.060546875 vl: 2.8984375 77.98 % [======================================>------------] 3509/4500 \t used:3806s eta:1074 sss11 sWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.015625 vl: 3.228515625 100.00 % [==================================================>] 4500/4500 \t used:4881s eta:0 s ssssss\n",
      "Epoch 27\n",
      "tl: 2.63671875 vl: 2.798828125 96.91 % [================================================>--] 4361/4500 \t used:4733s eta:150 sssssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.8046875 vl: 2.01953125 100.00 % [==================================================>] 4500/4500 \t used:4884s eta:0 sssss\n",
      "Epoch 28\n",
      "tl: 2.349609375 vl: 2.875 8.00 % [====>----------------------------------------------] 360/4500 \t used:390s eta:4492 s4502 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.322265625 vl: 2.2890625 100.00 % [==================================================>] 4500/4500 \t used:4882s eta:0 sss sss s\n",
      "Epoch 29\n",
      "tl: 2.5625 vl: 2.60546875 16.02 % [========>------------------------------------------] 721/4500 \t used:787s eta:4125 s125 sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.150390625 vl: 2.669921875 100.00 % [==================================================>] 4500/4500 \t used:4887s eta:0 s7 sss\n",
      "Epoch 30\n",
      "tl: 1.8310546875 vl: 2.62109375 15.93 % [=======>-------------------------------------------] 717/4500 \t used:779s eta:4110 sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.205078125 vl: 2.580078125 100.00 % [==================================================>] 4500/4500 \t used:4891s eta:0 ssssss\n",
      "Epoch 31\n",
      "tl: 2.6796875 vl: 2.6015625 5.49 % [==>------------------------------------------------] 247/4500 \t used:267s eta:4608 ss9 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.55859375 vl: 2.810546875 100.00 % [==================================================>] 4500/4500 \t used:4884s eta:0 sssssss\n",
      "Epoch 32\n",
      "tl: 2.26953125 vl: 1.97265625 27.58 % [=============>-------------------------------------] 1241/4500 \t used:1346s eta:3534 ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.625 vl: 2.537109375 100.00 % [==================================================>] 4500/4500 \t used:4887s eta:0 s1 s2 sss7 s\n",
      "Epoch 33\n",
      "tl: 2.609375 vl: 2.873046875 48.18 % [========================>--------------------------] 2168/4500 \t used:2351s eta:2528 s0 ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 3.130859375 vl: 2.412109375 100.00 % [==================================================>] 4500/4500 \t used:4878s eta:0 sssss\n",
      "Epoch 34\n",
      "tl: 2.5 vl: 2.73046875 4.62 % [==>------------------------------------------------] 208/4500 \t used:225s eta:4659 s662 ssssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.89453125 vl: 3.5 100.00 % [==================================================>] 4500/4500 \t used:4888s eta:0 s:1 s2 sss sss\n",
      "Epoch 35\n",
      "tl: 3.458984375 vl: 2.458984375 33.44 % [================>----------------------------------] 1505/4500 \t used:1635s eta:3254 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 1.3974609375 vl: 2.560546875 100.00 % [==================================================>] 4500/4500 \t used:4877s eta:0 ss ss\n",
      "Epoch 36\n",
      "tl: 3.203125 vl: 2.09375 17.53 % [========>------------------------------------------] 789/4500 \t used:854s eta:4018 ss ss sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.244140625 vl: 2.92578125 100.00 % [==================================================>] 4500/4500 \t used:4883s eta:0 s sss ss\n",
      "Epoch 37\n",
      "tl: 2.076171875 vl: 1.9775390625 30.89 % [===============>-----------------------------------] 1390/4500 \t used:1512s eta:3382 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.439453125 vl: 2.404296875 100.00 % [==================================================>] 4500/4500 \t used:4890s eta:0 ssss s s\n",
      "Epoch 38\n",
      "tl: 3.017578125 vl: 2.06640625 39.16 % [===================>-------------------------------] 1762/4500 \t used:1915s eta:2976 sssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 3.015625 vl: 2.794921875 100.00 % [==================================================>] 4500/4500 \t used:4888s eta:0 sssssssss\n",
      "Epoch 39\n",
      "tl: 2.396484375 vl: 2.802734375 15.29 % [=======>-------------------------------------------] 688/4500 \t used:746s eta:4134 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.392578125 vl: 2.47265625 100.00 % [==================================================>] 4500/4500 \t used:4888s eta:0 sss2 ss\n",
      "Epoch 40\n",
      "tl: 2.28125 vl: 3.181640625 76.53 % [======================================>------------] 3444/4500 \t used:3741s eta:1147 s48 s3 ssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.48828125 vl: 2.97265625 100.00 % [==================================================>] 4500/4500 \t used:4889s eta:0 sssssss\n",
      "Epoch 41\n",
      "tl: 3.0546875 vl: 2.236328125 92.89 % [==============================================>----] 4180/4500 \t used:4540s eta:347 sssss sWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.078125 vl: 3.486328125 100.00 % [==================================================>] 4500/4500 \t used:4888s eta:0 s sss s\n",
      "Epoch 42\n",
      "tl: 2.21875 vl: 2.734375 47.89 % [=======================>---------------------------] 2155/4500 \t used:2341s eta:2547 s s s sssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.421875 vl: 2.46875 100.00 % [==================================================>] 4500/4500 \t used:4886s eta:0 s1 sss sssss\n",
      "Epoch 43\n",
      "tl: 3.3828125 vl: 3.5703125 81.24 % [========================================>----------] 3656/4500 \t used:3970s eta:916 s s ssssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.05078125 vl: 2.8828125 100.00 % [==================================================>] 4500/4500 \t used:4889s eta:0 s ssssss\n",
      "Epoch 44\n",
      "tl: 2.328125 vl: 2.66015625 20.29 % [==========>----------------------------------------] 913/4500 \t used:992s eta:3899 s1 ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.390625 vl: 2.525390625 100.00 % [==================================================>] 4500/4500 \t used:4882s eta:0 ss4 sssss\n",
      "Epoch 45\n",
      "tl: 2.048828125 vl: 2.708984375 47.73 % [=======================>---------------------------] 2148/4500 \t used:2331s eta:2553 sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.421875 vl: 2.96875 100.00 % [==================================================>] 4500/4500 \t used:4887s eta:0 ss s1 ss ssss\n",
      "Epoch 46\n",
      "tl: 2.888671875 vl: 3.1171875 1.24 % [>--------------------------------------------------] 56/4500 \t used:60s eta:4783 sssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.435546875 vl: 2.671875 100.00 % [==================================================>] 4500/4500 \t used:4884s eta:0 s s ssss s\n",
      "Epoch 47\n",
      "tl: 2.841796875 vl: 3.037109375 17.47 % [========>------------------------------------------] 786/4500 \t used:853s eta:4031 ss sWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.216796875 vl: 2.521484375 100.00 % [==================================================>] 4500/4500 \t used:4882s eta:0 ssss s\n",
      "Epoch 48\n",
      "tl: 2.388671875 vl: 1.99609375 55.22 % [===========================>-----------------------] 2485/4500 \t used:2696s eta:2186 ssssWarning: NaN or Inf found in input tensor.\n",
      "tl: 2.66796875 vl: 3.595703125 68.18 % [==================================>----------------] 3068/4500 \t used:3329s eta:1554 ssss"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bec739c76db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             ),\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         )\n\u001b[1;32m     19\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bd/wfmrl/projects/ChineseAiDungeon/tf2gpt/model.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data, mask)\u001b[0m\n\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         ret = {\n\u001b[1;32m    312\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2248\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2252\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1895\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m             \u001b[0msub_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1897\u001b[0;31m             extra_variables=self._trainable_weights))\n\u001b[0m\u001b[1;32m   1898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mgather_trainable_weights\u001b[0;34m(trainable, sub_layers, extra_variables)\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m   trainable_extra_variables = [\n\u001b[1;32m    273\u001b[0m       v for v in extra_variables if v.trainable]\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0msub_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         extra_variables=self._self_extra_variables)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mgather_trainable_weights\u001b[0;34m(trainable, sub_layers, extra_variables)\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m   trainable_extra_variables = [\n\u001b[1;32m    273\u001b[0m       v for v in extra_variables if v.trainable]\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \"\"\"\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m       \u001b[0mchildren_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchildren_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2842\u001b[0m       return list(\n\u001b[1;32m   2843\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2844\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2842\u001b[0m       return list(\n\u001b[1;32m   2843\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2844\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \"\"\"\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m       \u001b[0mchildren_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchildren_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2842\u001b[0m       return list(\n\u001b[1;32m   2843\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2844\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2842\u001b[0m       return list(\n\u001b[1;32m   2843\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2844\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m       \u001b[0mchildren_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchildren_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_dedup_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2962\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2963\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m         \u001b[0;31m# Track the Variable's identity to avoid __eq__ issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/object_identity.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/object_identity.py\u001b[0m in \u001b[0;36m_wrap_key\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ObjectIdentityWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/object_identity.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, wrapped)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0m__slots__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_wrapped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('log/finetune')\n",
    "n_iter = 0\n",
    "train_loss = 100\n",
    "val_loss = 100\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    pb = ProgressBar(len(train_recoard))\n",
    "    pb.startjob()\n",
    "    for x,m in data_generator_json(train_recoard):\n",
    "    #for x in data_generator_content(texts[528:],sample_len=500,batch_size=2):\n",
    "        n_iter += 1\n",
    "        ret = gpt.train_step(\n",
    "            (\n",
    "                tf.constant(x[:,:-1]),\n",
    "                tf.constant(x[:,1:])\n",
    "            ),\n",
    "            mask=m[:,1:],\n",
    "        )\n",
    "        writer.add_scalar(\"train/loss\", ret[\"loss\"].numpy(), n_iter)\n",
    "        train_loss = ret[\"loss\"].numpy()\n",
    "        if n_iter % 10 == 0:\n",
    "            valid_x,valid_m = valid_gen.__next__()\n",
    "            ret = gpt.eval_step(\n",
    "            (\n",
    "                tf.constant(valid_x[:,:-1]),\n",
    "                tf.constant(valid_x[:,1:])),\n",
    "                mask=valid_m[:,1:],\n",
    "            )\n",
    "            writer.add_scalar(\"test/loss\", ret[\"loss\"].numpy(), n_iter)\n",
    "            val_loss = ret[\"loss\"].numpy()\n",
    "        pb.info = f\"tl: {train_loss} vl: {val_loss}\"\n",
    "        pb.complete(1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories[0].to_dungeon_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "99 你 是 一个 公司老总 , 你 事业 正在 上升期 , 你 娶 了 一个 美丽 的 妻子 \n",
      " >   你 走进 你 的 家门 \n",
      " 你 是 一个 公司老总 , 你 事业 正在 上升期 , 你 娶 了 一个 美丽 的 妻子 \n",
      " >   \n",
      " 你 的 妻子 回来 了 \n",
      " >   \n",
      " 你 的 妻子 回来 了 \n",
      " >   \n",
      " 你 的 父母 了 \n",
      " >   \n",
      " 你 的 父母 了 \n",
      " \n",
      " >   \n",
      " 你 的 父母 了 \n",
      " \n",
      " >   \n",
      " 他们\n"
     ]
    }
   ],
   "source": [
    "q = f'''你是一个公司老总，你事业正在上升期，你娶了一个美丽的妻子\n",
    "> 你走进你的家门\n",
    "'''\n",
    "ids = cbpe.encode(q)\n",
    "#print(ids)\n",
    "#print(\"+\" * 20)\n",
    "for i in range(100):\n",
    "    output = gpt(tf.constant([ids]))\n",
    "    nid = np.argmax(output[0, -1])\n",
    "    ids += [nid]\n",
    "    print(i)\n",
    "    \n",
    "print(i,cbpe.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.save_weights('./gpt_weight_pretrain/tmp_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model (restart kernel here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tf2gpt.model import GPT\n",
    "from utils.story_util import Story,Stories\n",
    "from utils.progress_bar import ProgressBar\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import random\n",
    "tf.keras.backend.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gpt2_tokenizer import GPT2Tokenizer\n",
    "cbpe = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16 <dtype: 'float16'> True\n"
     ]
    }
   ],
   "source": [
    "#with mirrored_strategy.scope():\n",
    "gpt = GPT(\n",
    "    vocab_size=30_000,\n",
    "    layer_size=32,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=2560,\n",
    "    num_attention_heads=32,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0\n",
    ")\n",
    "gpt.load_weights('./gpt_weight_pretrain/tmp_weight')\n",
    "\n",
    "#input_x = tf.keras.layers.Input((499,), dtype=tf.int32)\n",
    "#outputs = gpt_origin(input_x)\n",
    "\n",
    "#gpt = tf.keras.Model(inputs=input_x, outputs=outputs)\n",
    "#gpt = multi_gpu_model(gpt, gpus=8)\n",
    "\n",
    "print(tf.keras.backend.floatx(), tf.float16, tf.keras.backend.floatx() == tf.float16)\n",
    "if tf.keras.backend.floatx() == tf.float16:\n",
    "    for x in gpt.weights:\n",
    "        assert x.dtype == tf.float16\n",
    "\n",
    "\n",
    "gpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.563 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 今天天气 不错 \n",
      "1 今天天气 不错 ,\n",
      "2 今天天气 不错 , 不如\n",
      "3 今天天气 不错 , 不如 出去\n",
      "4 今天天气 不错 , 不如 出去 走走\n",
      "5 今天天气 不错 , 不如 出去 走走 吧\n",
      "6 今天天气 不错 , 不如 出去 走走 吧 \n",
      "7 今天天气 不错 , 不如 出去 走走 吧 。\n",
      "8 今天天气 不错 , 不如 出去 走走 吧 。 \n",
      "9 今天天气 不错 , 不如 出去 走走 吧 。 ”\n"
     ]
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气不错')\n",
    "\n",
    "for i in range(10):\n",
    "    output = gpt(tf.constant([ids]))\n",
    "    nid = np.argmax(output[0, -1])\n",
    "    ids += [nid]\n",
    "    print(i, cbpe.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gather(a, b):\n",
    "    return tf.gather(a, b, batch_dims=1)\n",
    "\n",
    "def penalize_used(logits, output,penalize):\n",
    "    # I want to change the indices of logits wherever the index is found in output\n",
    "    change_tensor = tf.zeros_like(logits, dtype=logits.dtype)\n",
    "    unique = tf.unique(output[0])[0]\n",
    "    ones = tf.ones_like(unique, dtype=unique.dtype)\n",
    "    indices = tf.expand_dims(unique, 1)\n",
    "    updates = tf.scatter_nd(indices, ones, [logits.shape[1]])\n",
    "    bool_tensor = tf.expand_dims(tf.cast(updates, tf.bool), 0)\n",
    "    return tf.compat.v1.where(bool_tensor, logits * penalize, logits)\n",
    "\n",
    "def top_p_sample(logits, inputs, num_samples=1, p=0.9,k=40,temperature=0.4,penalize=0.4):\n",
    "    logits = penalize_used(logits,inputs,penalize)\n",
    "    batch_size, vocab_size = logits.shape\n",
    "    probs = tf.nn.softmax(logits / temperature, axis=-1)\n",
    "    # [batch_size, vocab_perm]\n",
    "    indices = tf.argsort(probs, direction='DESCENDING')\n",
    "    cumulative_probabilities = tf.math.cumsum(batch_gather(probs, indices), axis=-1, exclusive=False)\n",
    "    top_k_prob = cumulative_probabilities[0][k]\n",
    "\n",
    "    # find the top pth index to cut off. careful we don't want to cutoff everything!\n",
    "    # result will be [batch_size, vocab_perm]\n",
    "    p_expanded = p if isinstance(p, float) else p#[:, None]\n",
    "    exclude_mask = tf.logical_not(\n",
    "        tf.logical_or(\n",
    "            tf.logical_and(\n",
    "                cumulative_probabilities < p_expanded, \n",
    "                cumulative_probabilities < top_k_prob\n",
    "            ),\n",
    "            tf.range(vocab_size)[None] < 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # OPTION A - sample in the sorted space, then unsort.\n",
    "    logits_to_use = batch_gather(logits, indices) - tf.cast(exclude_mask, tf.float16) * 1e4\n",
    "    sample_perm = tf.random.categorical(logits=logits_to_use, num_samples=num_samples)\n",
    "    sample = batch_gather(indices, sample_perm)\n",
    "\n",
    "    return tf.cast(sample, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def serve(inputs):\n",
    "    return gpt(inputs, kv_cache=None, use_cache=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def serve_cache(inputs, kv_cache):\n",
    "    return gpt(inputs, kv_cache=kv_cache, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_concrete = serve.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\")\n",
    ")\n",
    "\n",
    "layer_size = 32\n",
    "attention_head = 32\n",
    "embedding_size = 2560\n",
    "\n",
    "serve_cache_concrete = serve_cache.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\"),\n",
    "    tf.TensorSpec(shape=[\n",
    "        layer_size, None, 2, attention_head,\n",
    "        None, embedding_size // attention_head\n",
    "    ], dtype=tf.float16, name=\"kv_cache\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = serve_concrete(\n",
    "    tf.constant([[1]], tf.int64)\n",
    ")\n",
    "r2 = serve_cache_concrete(\n",
    "    tf.constant([[1]], tf.int64),\n",
    "    r[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample(initial_inputs, length, p,k,temperature,penalize, num_samples=1):\n",
    "    layer_size = 32\n",
    "    embedding_size = 2560\n",
    "    attention_head = 32\n",
    "\n",
    "    i = tf.constant(0, dtype=tf.int64)\n",
    "    initial_logits, kv_cache = serve(initial_inputs)\n",
    "    inputs = top_p_sample(initial_logits[:, -1, :],initial_inputs, p=p,k=k,temperature=temperature,penalize=penalize)\n",
    "    stores = tf.concat([initial_inputs, inputs], axis=1)\n",
    "\n",
    "    def _cond(i, inputs, kv_cache, stores):\n",
    "        return i < length\n",
    "\n",
    "    def _body(i, inputs, kv_cache, stores):\n",
    "        new_logits, new_kv_cache = serve_cache(inputs, kv_cache)\n",
    "        \n",
    "        new_inputs = top_p_sample(new_logits[:, -1, :],inputs, p=p,k=k,temperature=temperature,penalize=penalize)\n",
    "        new_stores = tf.concat([stores, new_inputs], axis=-1)\n",
    "        new_kv_cache = tf.concat([\n",
    "            kv_cache,\n",
    "            new_kv_cache\n",
    "        ], axis=-2)\n",
    "        new_i = i + 1\n",
    "        return [new_i, new_inputs, new_kv_cache, new_stores]\n",
    "\n",
    "    result = tf.while_loop(\n",
    "        _cond, _body,\n",
    "        loop_vars=[i, inputs, kv_cache, stores],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape(None),\n",
    "            tf.TensorShape([None, None]),\n",
    "            tf.TensorShape([\n",
    "                layer_size, None, 2,\n",
    "                attention_head, None,\n",
    "                embedding_size // attention_head\n",
    "            ]),\n",
    "            tf.TensorShape([\n",
    "                None, None\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "    return result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./result_weights/tmp_weight/assets\n"
     ]
    }
   ],
   "source": [
    "gpt.save('./result_weights/tmp_weight', include_optimizer=False, signatures={\n",
    "    'serving_default': sample.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\"),\n",
    "        tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"length\"),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.float16, name=\"p\"),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.int64, name=\"k\"),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.float16, name=\"temperature\"),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.float16, name=\"penalize\")\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  837   259   497   788     8     9  4609    64  1555 14981     8    12\n",
      "      8    34     8]], shape=(1, 15), dtype=int64)\n",
      "今天天气 不错 , 正好 可以 出去 走走 。 ” \n"
     ]
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气不错')\n",
    "\n",
    "ret = sample(\n",
    "    tf.constant([ids], dtype=tf.int64), # inp\n",
    "    tf.constant(10, dtype=tf.int64), #length\n",
    "    tf.constant(0.9, dtype=tf.float16), #p\n",
    "    tf.constant(40, dtype=tf.int64), #k\n",
    "    tf.constant(0.4, dtype=tf.float16), # temperature\n",
    "    tf.constant(0.4, dtype=tf.float16) # penalize\n",
    ")\n",
    "print(ret)\n",
    "print(cbpe.decode(ret.numpy().tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 23564\n",
      "drwxr-xr-x 2 tiger tiger     4096 Jul  5 15:21 assets\n",
      "-rw-r--r-- 1 tiger tiger 24117909 Jul 12 10:34 saved_model.pb\n",
      "drwxr-xr-x 3 tiger tiger     4096 Jul 12 10:34 variables\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./result_weights/tmp_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read deploy model (you should restart kernel and go from here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tf2gpt.model import GPT\n",
    "from utils.story_util import Story,Stories\n",
    "from utils.progress_bar import ProgressBar\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import random\n",
    "from utils.gpt2_tokenizer import GPT2Tokenizer\n",
    "import tensorflow_hub as hub\n",
    "cbpe = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = hub.load('./result_weights/tmp_weight/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gpt(tokenizer, gpt, sentence, number=1, length=20, p=0.9,k=40,temperature=0.4,penalize=0.85):\n",
    "    inputs = tf.constant([tokenizer.encode(sentence)] * number, dtype=tf.int64)\n",
    "    length = tf.constant(length, dtype=tf.int64)\n",
    "    p = tf.constant(p, dtype=tf.float16)\n",
    "    k = tf.constant(k, dtype=tf.int64)\n",
    "    temperature = tf.constant(temperature, dtype=tf.float16)\n",
    "    penalize = tf.constant(penalize, dtype=tf.float16)\n",
    "    ret = gpt.signatures['serving_default'](\n",
    "        inp=inputs, \n",
    "        length=length, \n",
    "        p=p,\n",
    "        k=k,\n",
    "        temperature=temperature,\n",
    "        penalize=penalize)['output_0']\n",
    "    return [\n",
    "        tokenizer.decode(s[len(inputs[0]):]).replace(' ', '')\n",
    "        for s in ret.numpy()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q = f'''段誉和王语嫣面面相对，呼吸可闻，虽身处污泥，心中却充满了喜乐之情，谁也没想到要爬出井去。两人同时慢慢的伸出手来，四手相握，心意相通。'''\n",
    "ret = sample_gpt(cbpe, gpt, q, 1, 500, p=0.9,k=16,temperature=1,penalize=0.75)\n",
    "for x in ret:\n",
    "    print(x)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Story():\n",
    "    def __init__(self,beginning,story_max_len=200,context_len=12):\n",
    "        self.story = [beginning]\n",
    "        self.story_max_len = story_max_len\n",
    "        self.context_len = context_len\n",
    "        \n",
    "    def response_quality_ok(self,response):\n",
    "        if len(response) < 20:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def action(self,action):\n",
    "        action_str = \"> 你\" + action\n",
    "        q = \"\".join(self.story[-self.context_len:]) + \"\\n\" + action_str + \"\\n\"\n",
    "        self.story.append(\"你\" + action)\n",
    "        response = \"\"\n",
    "        for i in range(3):\n",
    "            response = sample_gpt(cbpe, gpt, q, 1, 150,p=0.9,k=16,temperature=1,penalize=0.75)[0]\n",
    "            response = '。'.join(response.split(\"。\")[:-1]) + \"。\"\n",
    "            if(self.response_quality_ok(response) == True):\n",
    "                break\n",
    "            else:\n",
    "                print(\"response quality check failed,generating another one\")\n",
    "        \n",
    "        #responsesp = response.split(\"。\")\n",
    "        #if(len(responsesp) > 2):\n",
    "        #    response = ''.join(responsesp[:-1])\n",
    "        #else:\n",
    "        #    response = response.split(\">\")[0]\n",
    "        #response = \"。\".join([:-1])\n",
    "        #response = response.split(\">\")[0].strip()\n",
    "        #response = \"\\n\".join(response.split(\"\\n\")[:-1])\n",
    "        self.story.append(response)\n",
    "        \n",
    "    def interactive(self):\n",
    "        print(\"\\n\".join(self.story))\n",
    "        while True:\n",
    "            action = input(\"> 你\")\n",
    "            self.action(action)\n",
    "            print(self.story[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = f'''你在树林里冒险，指不定会从哪里蹦出来一些奇怪的东西，你握紧手上的手枪，希望这次冒险能够找到一些值钱的东西'''\n",
    "story = Story(begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你在树林里冒险，指不定会从哪里蹦出来一些奇怪的东西，你握紧手上的手枪，希望这次冒险能够找到一些值钱的东西\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 前往一个城镇\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你已经厌倦了这个地方,你现在需要尽快逃离这里,而这会为你的安全提供一些帮助。你沿着街道一直走,偶尔看到一两个人在街道上移动,你也没太在意,他们的行动似乎表明他们在等待。“你怎么知道,你现在看到了什么?”你问一个人,这个人正在抱怨他的同伴。“如果他们都在树林里等待着什么,那他们为什么要在这里等着我?”你问。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 走进一家酒馆\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当你进入酒馆的时候,你立刻开始感到不安,因为酒馆里有人在交谈,你很快就会被淹没在他们的谈话中。“这里有没有一个,”你喊道,但在你说“不是”之前,“他们都不,”一个人回答,“他们是在谈论昨天晚上发生的事情,我看到他们在外面的人。”“好吧,那是什么?”你问他,他看起来似乎很惊讶。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 询问昨天发生了什么？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“昨天晚上,我听到有人在外面。”“什么声音?”“我没有看到任何人,”你告诉他,“当他们开始讨论昨天晚上的事情时,他们似乎都在讨论某件事,”他说,“然后他们听到了。”“你看到了什么?”“没有任何事情发生,但我听到了你的声音,我认为他们在外面的人听到了你的声音。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 真无聊，在吧台上点一杯鸡尾酒\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "她耸了耸肩,示意你不要点酒,你点一些你最喜欢的甜酒和一些奶酪,然后回到酒吧。你想知道“什么”是什么,但你知道你必须快点去睡觉。当你离开酒吧时,一种奇怪的感觉向你袭来,似乎有什么东西从你的身体里冒出来,在您离开之前,这种感觉更加强烈。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 走进旅馆开一间房间\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "几分钟后你回到了旅馆,走进你的房间,你关上了身后的门,因为你不想让自己从门的另一边看到任何东西。你闭上眼睛,在睡梦中,你进入了一个不可思议的世界。你感觉到了一种不可思议的平静,在黑暗中,你的周围是一片寂静,在这种宁静中,你感觉到了另一个世界的平静。你睁开眼睛,你看到你躺在一个旅馆里,你听到外面传来你自己的声音。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 休息一晚上之后醒来\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "睡眠是一种享受,而现在睡眠是多么糟糕的事情。你感觉自己在发烧,你看到了你的身体上所有的痛苦,当你终于感觉到疼痛时,你的头很疼,你在这里的每一个晚上都在发烧,你希望能让自己睡觉,你希望在第二天早上醒来,但还是让自己入睡。你希望睡觉可以让你的头痛减少,让你能在睡眠中继续做梦。你希望睡眠的时候不要被噩梦惊醒,你希望睡眠的时候不会有噩梦的惊醒。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 下楼吃点感冒药\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当你下楼吃感冒药时,你感觉不到任何冰冷的感觉,当你下楼时,你感觉很温暖,感觉不到任何恐惧。你终于吃完了感冒药,你感到胃里好像有东西在颤抖,你开始感觉到寒冷,但你仍然感觉很好。你的喉咙似乎被堵住了,但你仍然在打喷嚏,让你感觉很好。你的喉咙开始灼热,当你感觉有点冷的时候,它已经是你唯一的感觉。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 走出门观察小镇\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早晨在街道上游荡是很正常的,因为你只是想找到一些人,然后把所有人都吓一跳,你希望在任何人离开的时候都是这样。你在街上走着,你可以感觉到有什么东西在你的身上起起伏伏,感觉就像有东西在你的身体里一样。你走进酒馆当你进入酒馆的时候,你立刻开始感到不安,因为酒馆里有人在交谈,你很快就会被淹没在他们的交谈中。\n"
     ]
    }
   ],
   "source": [
    "story.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
