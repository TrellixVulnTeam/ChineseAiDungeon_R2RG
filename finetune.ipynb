{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tf2gpt.model import GPT\n",
    "from utils.story_util import Story,Stories\n",
    "from utils.progress_bar import ProgressBar\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16 <dtype: 'float16'> True\n"
     ]
    }
   ],
   "source": [
    "#with mirrored_strategy.scope():\n",
    "gpt = GPT(\n",
    "    vocab_size=30_000,\n",
    "    layer_size=32,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=2560,\n",
    "    num_attention_heads=32,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0,\n",
    "    train_size=499\n",
    ")\n",
    "gpt.load_weights('./gpt_weight_pretrain/weight_fp16_pretrained_norm')\n",
    "\n",
    "#input_x = tf.keras.layers.Input((499,), dtype=tf.int32)\n",
    "#outputs = gpt_origin(input_x)\n",
    "\n",
    "#gpt = tf.keras.Model(inputs=input_x, outputs=outputs)\n",
    "#gpt = multi_gpu_model(gpt, gpus=8)\n",
    "\n",
    "print(tf.keras.backend.floatx(), tf.float16, tf.keras.backend.floatx() == tf.float16)\n",
    "if tf.keras.backend.floatx() == tf.float16:\n",
    "    for x in gpt.weights:\n",
    "        assert x.dtype == tf.float16\n",
    "\n",
    "\n",
    "gpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gpt2_tokenizer import GPT2Tokenizer\n",
    "cbpe = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.573 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[837, 259, 497, 57, 8, 237]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气还行')\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[837, 259, 497, 57, 8, 237]\n",
      "++++++++++++++++++++\n",
      "(6, 30000)\n",
      "0 今天天气 还 行 \n",
      "[    8   497    46   788 25753     8]\n",
      "气 很 不错暖和 \n",
      "------------------------------\n",
      "(7, 30000)\n",
      "1 今天天气 还 行 ,\n",
      "[    8   497    46   788 25753     8     9]\n",
      "气 很 不错暖和 ,\n",
      "------------------------------\n",
      "(8, 30000)\n",
      "2 今天天气 还 行 , 但\n",
      "[    8   497    46   788 25753     8     9    51]\n",
      "气 很 不错暖和 , 但\n",
      "------------------------------\n",
      "(9, 30000)\n",
      "3 今天天气 还 行 , 但是\n",
      "[    8   497    46   788 25753     8     9    51    35]\n",
      "气 很 不错暖和 , 但是\n",
      "------------------------------\n",
      "(10, 30000)\n",
      "4 今天天气 还 行 , 但是 我\n",
      "[    8   497    46   788 25753     8     9    51    35    16]\n",
      "气 很 不错暖和 , 但是 我\n",
      "------------------------------\n",
      "(11, 30000)\n",
      "5 今天天气 还 行 , 但是 我 不\n",
      "[    8   497    46   788 25753     8     9    51    35    16    24]\n",
      "气 很 不错暖和 , 但是 我 不\n",
      "------------------------------\n",
      "(12, 30000)\n",
      "6 今天天气 还 行 , 但是 我 不想\n",
      "[    8   497    46   788 25753     8     9    51    35    16    24   404]\n",
      "气 很 不错暖和 , 但是 我 不想\n",
      "------------------------------\n",
      "(13, 30000)\n",
      "7 今天天气 还 行 , 但是 我 不想 在\n",
      "[    8   497    46   788 25753     8     9    51    35    16    24   404\n",
      "    18]\n",
      "气 很 不错暖和 , 但是 我 不想 在\n",
      "------------------------------\n",
      "(14, 30000)\n",
      "8 今天天气 还 行 , 但是 我 不想 在 这\n",
      "[    8   497    46   788 25753     8     9    51    35    16    24   404\n",
      "    18    25]\n",
      "气 很 不错暖和 , 但是 我 不想 在 这\n",
      "------------------------------\n",
      "(15, 30000)\n",
      "9 今天天气 还 行 , 但是 我 不想 在 这里\n",
      "[    8   497    46   788 25753     8     9    51    35    16    24   404\n",
      "    18    25   154]\n",
      "气 很 不错暖和 , 但是 我 不想 在 这里\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_basic_logic():\n",
    "    ids = cbpe.encode('今天天气还行')\n",
    "    print(ids)\n",
    "    print(\"+\" * 20)\n",
    "    for i in range(10):\n",
    "        output = gpt(tf.constant([ids]))\n",
    "        print(output[0].shape)\n",
    "        nid = np.argmax(output[0, -1])\n",
    "        ids += [nid]\n",
    "        print(i, cbpe.decode(ids))\n",
    "        print(np.argmax(output[0],axis=-1))\n",
    "        print(cbpe.decode(np.argmax(output[0],axis=-1)))\n",
    "        print('-' * 30)\n",
    "test_basic_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(learning_rate=6e-4,\n",
    "                      warmup_steps=20_0000,\n",
    "                      decay_steps=200_0000,\n",
    "                      alpha=0.0):\n",
    "    def decayed_learning_rate(step=1):\n",
    "        if step <= warmup_steps:\n",
    "            mult = step / float(warmup_steps)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (decay_steps - warmup_steps)\n",
    "            mult = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "            mult = max(0.1, mult)\n",
    "        return learning_rate * mult\n",
    "    return decayed_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = Stories(\"./labeled_data/advanture_translated/processed_translated_story.txt\").stories\n",
    "#stories = stories[:50]\n",
    "data_folder = \"./labeled_data/\"\n",
    "txt_files = [(data_folder + i) for i in os.listdir(data_folder) if \"txt\" in i]\n",
    "#stories = stories[:10]\n",
    "stories += [Story(\"\",\"\").from_file(i) for i in txt_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stories = Stories(\"./labeled_data/advanture_translated/processed_translated_story_valid.txt\").stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 35)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stories),len(valid_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def data_generator(stories, batch_size=4,sample_len=200,inf=False):\n",
    "    while True:\n",
    "        batch_data = []\n",
    "        tmp_stories = copy.copy(stories)\n",
    "        random.shuffle(tmp_stories)\n",
    "        for i,one_story in enumerate(tmp_stories):\n",
    "            story_content = one_story.to_dungeon_format()\n",
    "            story_content = story_content.replace(\"<start>\\n\",\"\")\n",
    "            story_content = story_content.replace(\"\\n<end>\",\"\")\n",
    "            story_content = story_content.replace(\"\\n<end>\",\"\")\n",
    "            story_content = story_content.replace(\" \",\"\")\n",
    "            ids = cbpe.encode(story_content)\n",
    "            while ids:\n",
    "                sample = ids[:sample_len]\n",
    "                ids = ids[sample_len:]\n",
    "                if len(sample) < sample_len:\n",
    "                    sample += [0 for i in range((sample_len - len(sample)))]\n",
    "                batch_data.append(sample)\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    yield np.asarray(batch_data)\n",
    "                    batch_data = []\n",
    "        if not inf:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = data_generator(valid_stories,sample_len=500,batch_size=2,inf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 500)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_gen.__next__().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "tl: 2.86328125 vl: 2.86328125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1444s eta:9759 ssss\n",
      "Epoch 2\n",
      "tl: 2.416015625 vl: 2.646484375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1443s eta:9756 ss\n",
      "Epoch 3\n",
      "tl: 2.744140625 vl: 2.828125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1443s eta:9755 s3 ss\n",
      "Epoch 4\n",
      "tl: 2.671875 vl: 1.515625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1443s eta:9757 s8 ss ss\n",
      "Epoch 5\n",
      "tl: 2.5546875 vl: 2.85546875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1444s eta:9762 s sss\n",
      "Epoch 6\n",
      "tl: 2.98046875 vl: 2.4921875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1447s eta:9779 ss ss\n",
      "Epoch 7\n",
      "tl: 3.123046875 vl: 2.357421875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1446s eta:9777 ss\n",
      "Epoch 8\n",
      "tl: 2.533203125 vl: 2.45703125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1448s eta:9787 ssss\n",
      "Epoch 9\n",
      "tl: 2.525390625 vl: 2.7109375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1451s eta:9804 sssss\n",
      "Epoch 10\n",
      "tl: 2.63671875 vl: 2.638671875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1447s eta:9783 ssss\n",
      "Epoch 11\n",
      "tl: 2.3203125 vl: 2.7421875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1448s eta:9790 s s sss\n",
      "Epoch 12\n",
      "tl: 2.78515625 vl: 2.5703125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1448s eta:9788 ss ss\n",
      "Epoch 13\n",
      "tl: 2.509765625 vl: 2.734375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1447s eta:9783 s3 ss\n",
      "Epoch 14\n",
      "tl: 2.98046875 vl: 2.517578125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1448s eta:9786 sss\n",
      "Epoch 15\n",
      "tl: 2.779296875 vl: 1.3466796875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1448s eta:9789 s\n",
      "Epoch 16\n",
      "tl: 2.568359375 vl: 2.91796875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1448s eta:9784 sss\n",
      "Epoch 17\n",
      "tl: 2.748046875 vl: 2.85546875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1449s eta:9791 sss\n",
      "Epoch 18\n",
      "tl: 2.921875 vl: 2.69921875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1445s eta:9771 s sssss\n",
      "Epoch 19\n",
      "tl: 2.751953125 vl: 2.6796875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1448s eta:9787 s ss\n",
      "Epoch 20\n",
      "tl: 3.1328125 vl: 2.62109375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1448s eta:9784 s sss\n",
      "Epoch 21\n",
      "tl: 2.666015625 vl: 2.91015625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1446s eta:9773 sss\n",
      "Epoch 22\n",
      "tl: 2.2578125 vl: 2.61328125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1447s eta:9782 s ssss\n",
      "Epoch 23\n",
      "tl: 2.416015625 vl: 2.701171875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1440s eta:9736 sss\n",
      "Epoch 24\n",
      "tl: 2.72265625 vl: 2.517578125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1433s eta:9688 sss\n",
      "Epoch 25\n",
      "tl: 2.701171875 vl: 2.697265625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1435s eta:9702 ss\n",
      "Epoch 26\n",
      "tl: 2.734375 vl: 2.767578125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1433s eta:9688 s0 ss\n",
      "Epoch 27\n",
      "tl: 2.54296875 vl: 2.609375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1439s eta:9728 ssss s\n",
      "Epoch 28\n",
      "tl: 2.3671875 vl: 2.521484375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1441s eta:9741 s ss\n",
      "Epoch 29\n",
      "tl: 2.423828125 vl: 2.6640625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1438s eta:9722 ssss\n",
      "Epoch 30\n",
      "tl: 2.662109375 vl: 2.697265625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1433s eta:9689 ss\n",
      "Epoch 31\n",
      "tl: 2.798828125 vl: 2.833984375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1435s eta:9698 ss\n",
      "Epoch 32\n",
      "tl: 2.951171875 vl: 2.654296875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1435s eta:9699 ss\n",
      "Epoch 33\n",
      "tl: 2.548828125 vl: 2.556640625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1435s eta:9697 ss\n",
      "Epoch 34\n",
      "tl: 2.96484375 vl: 2.693359375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1439s eta:9729 sss\n",
      "Epoch 35\n",
      "tl: 2.802734375 vl: 2.95703125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1436s eta:9706 ssss\n",
      "Epoch 36\n",
      "tl: 2.494140625 vl: 2.369140625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1438s eta:9719 ss\n",
      "Epoch 37\n",
      "tl: 2.537109375 vl: 2.9921875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1434s eta:9695 ssss\n",
      "Epoch 38\n",
      "tl: 2.943359375 vl: 2.783203125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1433s eta:9684 s\n",
      "Epoch 39\n",
      "tl: 2.52734375 vl: 2.576171875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1433s eta:9686 sss\n",
      "Epoch 40\n",
      "tl: 2.828125 vl: 2.875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9673 sa:9671 ss\n",
      "Epoch 41\n",
      "tl: 2.3203125 vl: 2.796875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1435s eta:9699 sss1 ss\n",
      "Epoch 42\n",
      "tl: 2.408203125 vl: 2.630859375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1434s eta:9694 ss\n",
      "Epoch 43\n",
      "tl: 2.328125 vl: 2.54296875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9675 s sssss\n",
      "Epoch 44\n",
      "tl: 2.599609375 vl: 2.65625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1435s eta:9702 s9 sss\n",
      "Epoch 45\n",
      "tl: 2.82421875 vl: 2.642578125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1433s eta:9689 ssss\n",
      "Epoch 46\n",
      "tl: 2.70703125 vl: 2.84765625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1434s eta:9695 ssss\n",
      "Epoch 47\n",
      "tl: 2.43359375 vl: 2.6484375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1435s eta:9701 ss ss\n",
      "Epoch 48\n",
      "tl: 2.5078125 vl: 2.603515625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1434s eta:9690 sss\n",
      "Epoch 49\n",
      "tl: 2.525390625 vl: 2.572265625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1428s eta:9650 ss\n",
      "Epoch 50\n",
      "tl: 2.990234375 vl: 2.611328125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9657 s\n",
      "Epoch 51\n",
      "tl: 2.591796875 vl: 2.689453125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9658 ss\n",
      "Epoch 52\n",
      "tl: 2.728515625 vl: 2.673828125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9670 s\n",
      "Epoch 53\n",
      "tl: 2.763671875 vl: 2.732421875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9666 ss\n",
      "Epoch 54\n",
      "tl: 2.96484375 vl: 2.95703125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9675 ssss\n",
      "Epoch 55\n",
      "tl: 2.568359375 vl: 2.638671875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9660 s\n",
      "Epoch 56\n",
      "tl: 2.884765625 vl: 2.658203125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1428s eta:9650 ss\n",
      "Epoch 57\n",
      "tl: 2.85546875 vl: 2.107421875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9662 sss\n",
      "Epoch 58\n",
      "tl: 2.544921875 vl: 2.666015625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9673 ss\n",
      "Epoch 59\n",
      "tl: 2.935546875 vl: 2.802734375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1426s eta:9640 ss\n",
      "Epoch 60\n",
      "tl: 2.76953125 vl: 2.91015625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9671 sss\n",
      "Epoch 61\n",
      "tl: 2.5390625 vl: 2.716796875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1427s eta:9647 s s\n",
      "Epoch 62\n",
      "tl: 3.00390625 vl: 2.556640625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9663 ss\n",
      "Epoch 63\n",
      "tl: 2.599609375 vl: 2.50390625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1427s eta:9644 ss\n",
      "Epoch 64\n",
      "tl: 2.814453125 vl: 2.74609375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1428s eta:9653 ss\n",
      "Epoch 65\n",
      "tl: 2.822265625 vl: 2.61328125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1427s eta:9646 ss\n",
      "Epoch 66\n",
      "tl: 2.701171875 vl: 2.708984375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9668 s\n",
      "Epoch 67\n",
      "tl: 2.775390625 vl: 2.421875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1428s eta:9653 ss s\n",
      "Epoch 68\n",
      "tl: 2.439453125 vl: 2.13671875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1428s eta:9653 sss\n",
      "Epoch 69\n",
      "tl: 2.462890625 vl: 2.80859375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1427s eta:9645 ssss\n",
      "Epoch 70\n",
      "tl: 2.93359375 vl: 2.40234375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9670 s ss\n",
      "Epoch 71\n",
      "tl: 2.216796875 vl: 2.525390625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1432s eta:9682 ss\n",
      "Epoch 72\n",
      "tl: 2.80859375 vl: 2.8046875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9669 ssss\n",
      "Epoch 73\n",
      "tl: 2.73046875 vl: 2.66796875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9657 sss\n",
      "Epoch 74\n",
      "tl: 2.70703125 vl: 2.84765625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9667 sss\n",
      "Epoch 75\n",
      "tl: 2.73828125 vl: 2.26171875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1433s eta:9686 ssss\n",
      "Epoch 76\n",
      "tl: 2.94140625 vl: 2.662109375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1428s eta:9652 ss\n",
      "Epoch 77\n",
      "tl: 2.61328125 vl: 2.486328125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1427s eta:9648 s s\n",
      "Epoch 78\n",
      "tl: 2.431640625 vl: 2.6015625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1426s eta:9640 s ss\n",
      "Epoch 79\n",
      "tl: 2.552734375 vl: 2.5859375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9670 ssss\n",
      "Epoch 80\n",
      "tl: 3.095703125 vl: 2.693359375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9657 ss\n",
      "Epoch 81\n",
      "tl: 2.80859375 vl: 2.693359375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1427s eta:9646 ss\n",
      "Epoch 82\n",
      "tl: 2.66015625 vl: 2.54296875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9659 ssss\n",
      "Epoch 83\n",
      "tl: 2.31640625 vl: 2.7421875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9674 ss s\n",
      "Epoch 84\n",
      "tl: 2.328125 vl: 2.466796875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9664 s ss\n",
      "Epoch 85\n",
      "tl: 2.5 vl: 2.763671875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9662 s:9663 ss\n",
      "Epoch 86\n",
      "tl: 2.525390625 vl: 2.5625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1433s eta:9685 ss ssss\n",
      "Epoch 87\n",
      "tl: 2.650390625 vl: 2.31640625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9672 sss\n",
      "Epoch 88\n",
      "tl: 2.36328125 vl: 2.15625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9666 ss68 ss\n",
      "Epoch 89\n",
      "tl: 2.67578125 vl: 2.111328125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9674 sss\n",
      "Epoch 90\n",
      "tl: 2.728515625 vl: 2.15625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1432s eta:9678 s7 sss\n",
      "Epoch 91\n",
      "tl: 2.8203125 vl: 2.30859375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9661 s sss\n",
      "Epoch 92\n",
      "tl: 2.681640625 vl: 2.57421875 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9671 sss\n",
      "Epoch 93\n",
      "tl: 2.68359375 vl: 2.314453125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1428s eta:9654 sss\n",
      "Epoch 94\n",
      "tl: 3.037109375 vl: 2.87109375 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9665 ss\n",
      "Epoch 95\n",
      "tl: 2.505859375 vl: 2.775390625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9663 s\n",
      "Epoch 96\n",
      "tl: 2.150390625 vl: 2.611328125 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1430s eta:9669 ss\n",
      "Epoch 97\n",
      "tl: 2.751953125 vl: 2.56640625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1429s eta:9657 sss\n",
      "Epoch 98\n",
      "tl: 2.818359375 vl: 2.869140625 12.89 % [======>--------------------------------------------] 1289/10000 \t used:1431s eta:9671 s s\n",
      "Epoch 99\n",
      "tl: 3.0859375 vl: 2.50390625 8.35 % [====>----------------------------------------------] 835/10000 \t used:924s eta:10150 s sss"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('log/finetune')\n",
    "n_iter = 0\n",
    "train_loss = 100\n",
    "val_loss = 100\n",
    "for epoch in range(200):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    pb = ProgressBar(10000)\n",
    "    pb.startjob()\n",
    "    for x in data_generator(stories,sample_len=500,batch_size=2):\n",
    "    #for x in data_generator_content(texts[508:],sample_len=400,batch_size=2):\n",
    "        n_iter += 1\n",
    "        ret = gpt.train_step(\n",
    "        (\n",
    "            tf.constant(x[:,:-1]),\n",
    "            tf.constant(x[:,1:]))\n",
    "        )\n",
    "        writer.add_scalar(\"train/loss\", ret[\"loss\"].numpy(), n_iter)\n",
    "        train_loss = ret[\"loss\"].numpy()\n",
    "        if n_iter % 10 == 0:\n",
    "            valid_x = valid_gen.__next__()\n",
    "            ret = gpt.eval_step(\n",
    "            (\n",
    "                tf.constant(valid_x[:,:-1]),\n",
    "                tf.constant(valid_x[:,1:]))\n",
    "            )\n",
    "            writer.add_scalar(\"test/loss\", ret[\"loss\"].numpy(), n_iter)\n",
    "            val_loss = ret[\"loss\"].numpy()\n",
    "        pb.info = f\"tl: {train_loss} vl: {val_loss}\"\n",
    "        pb.complete(1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories[0].to_dungeon_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "99 你 是 一个 公司老总 , 你 事业 正在 上升期 , 你 娶 了 一个 美丽 的 妻子 \n",
      " >   你 走进 你 的 家门 \n",
      " 你 走进 你 的 家 , 你 的 妻子 正在 等 你 。 你 的 妻子 是 一个 美丽 的 女人 , 她 的 眼睛 是 蓝色 的 , 她 的 头发 是 棕色 的 , 她 的 皮肤 是 棕色 的 , 她 的 嘴唇 是 红色 的 , 她 的 鼻子 是 红色 的 , 她 的 嘴唇 是 红色 的 。 她 的 头发 是 棕色 的 , 她 的 眼睛 是 蓝色 的 , 她 的 皮肤 是\n"
     ]
    }
   ],
   "source": [
    "q = f'''你是一个公司老总，你事业正在上升期，你娶了一个美丽的妻子\n",
    "> 你走进你的家门\n",
    "'''\n",
    "ids = cbpe.encode(q)\n",
    "#print(ids)\n",
    "#print(\"+\" * 20)\n",
    "for i in range(100):\n",
    "    output = gpt(tf.constant([ids]))\n",
    "    nid = np.argmax(output[0, -1])\n",
    "    ids += [nid]\n",
    "    print(i)\n",
    "    \n",
    "print(i,cbpe.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.save_weights('./gpt_weight_pretrain/weight_fp16_200epoch_308stories_dungeonformat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.560 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 今天天气 不错 \n",
      "1 今天天气 不错 ,\n",
      "2 今天天气 不错 , 我\n",
      "3 今天天气 不错 , 我 想\n",
      "4 今天天气 不错 , 我 想 我\n",
      "5 今天天气 不错 , 我 想 我 可以\n",
      "6 今天天气 不错 , 我 想 我 可以 出去\n",
      "7 今天天气 不错 , 我 想 我 可以 出去 \n",
      "8 今天天气 不错 , 我 想 我 可以 出去 。\n",
      "9 今天天气 不错 , 我 想 我 可以 出去 。 \n"
     ]
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气不错')\n",
    "\n",
    "for i in range(10):\n",
    "    output = gpt(tf.constant([ids]))\n",
    "    nid = np.argmax(output[0, -1])\n",
    "    ids += [nid]\n",
    "    print(i, cbpe.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gather(a, b):\n",
    "    return tf.gather(a, b, batch_dims=1)\n",
    "\n",
    "\n",
    "def top_p_sample(logits, num_samples=1, p=0.95):\n",
    "    batch_size, vocab_size = logits.shape\n",
    "    probs = tf.nn.softmax(logits, axis=-1)\n",
    "    # [batch_size, vocab_perm]\n",
    "    indices = tf.argsort(probs, direction='DESCENDING')\n",
    "    cumulative_probabilities = tf.math.cumsum(batch_gather(probs, indices), axis=-1, exclusive=False)\n",
    "\n",
    "    # find the top pth index to cut off. careful we don't want to cutoff everything!\n",
    "    # result will be [batch_size, vocab_perm]\n",
    "    p_expanded = p if isinstance(p, float) else p[:, None]\n",
    "    exclude_mask = tf.logical_not(\n",
    "        tf.logical_or(cumulative_probabilities < p_expanded, tf.range(vocab_size)[None] < 1))\n",
    "\n",
    "    # OPTION A - sample in the sorted space, then unsort.\n",
    "    logits_to_use = batch_gather(logits, indices) - tf.cast(exclude_mask, tf.float16) * 1e4\n",
    "    sample_perm = tf.random.categorical(logits=logits_to_use, num_samples=num_samples)\n",
    "    sample = batch_gather(indices, sample_perm)\n",
    "\n",
    "    return tf.cast(sample, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def serve(inputs):\n",
    "    return gpt(inputs, kv_cache=None, use_cache=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def serve_cache(inputs, kv_cache):\n",
    "    return gpt(inputs, kv_cache=kv_cache, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_concrete = serve.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\")\n",
    ")\n",
    "\n",
    "layer_size = 32\n",
    "attention_head = 32\n",
    "embedding_size = 2560\n",
    "\n",
    "serve_cache_concrete = serve_cache.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\"),\n",
    "    tf.TensorSpec(shape=[\n",
    "        layer_size, None, 2, attention_head,\n",
    "        None, embedding_size // attention_head\n",
    "    ], dtype=tf.float16, name=\"kv_cache\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = serve_concrete(\n",
    "    tf.constant([[1]], tf.int64)\n",
    ")\n",
    "r2 = serve_cache_concrete(\n",
    "    tf.constant([[1]], tf.int64),\n",
    "    r[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample(initial_inputs, length):\n",
    "    layer_size = 32\n",
    "    embedding_size = 2560\n",
    "    attention_head = 32\n",
    "\n",
    "    i = tf.constant(0, dtype=tf.int64)\n",
    "    initial_logits, kv_cache = serve(initial_inputs)\n",
    "    inputs = top_p_sample(initial_logits[:, -1, :])\n",
    "    stores = tf.concat([initial_inputs, inputs], axis=1)\n",
    "\n",
    "    def _cond(i, inputs, kv_cache, stores):\n",
    "        return i < length\n",
    "\n",
    "    def _body(i, inputs, kv_cache, stores):\n",
    "        new_logits, new_kv_cache = serve_cache(inputs, kv_cache)\n",
    "        \n",
    "        new_inputs = top_p_sample(new_logits[:, -1, :])\n",
    "        new_stores = tf.concat([stores, new_inputs], axis=-1)\n",
    "        new_kv_cache = tf.concat([\n",
    "            kv_cache,\n",
    "            new_kv_cache\n",
    "        ], axis=-2)\n",
    "        new_i = i + 1\n",
    "        return [new_i, new_inputs, new_kv_cache, new_stores]\n",
    "\n",
    "    result = tf.while_loop(\n",
    "        _cond, _body,\n",
    "        loop_vars=[i, inputs, kv_cache, stores],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape(None),\n",
    "            tf.TensorShape([None, None]),\n",
    "            tf.TensorShape([\n",
    "                layer_size, None, 2,\n",
    "                attention_head, None,\n",
    "                embedding_size // attention_head\n",
    "            ]),\n",
    "            tf.TensorShape([\n",
    "                None, None\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "    return result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./result_weights/cpm-lm-tf2-fp16-fine4m-dungeon-format2day/assets\n"
     ]
    }
   ],
   "source": [
    "gpt.save('./result_weights/cpm-lm-tf2-fp16-fine4m-dungeon-format2day', include_optimizer=False, signatures={\n",
    "    'serving_default': sample.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\"),\n",
    "        tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"length\")\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  837   259   497   788     8     9   457    27     8 25427     8    12\n",
      "      8    34     8]], shape=(1, 15), dtype=int64)\n",
      "今天天气 不错 , 向 你 眨眼 。 ” \n"
     ]
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气不错')\n",
    "\n",
    "ret = sample(\n",
    "    tf.constant([ids], dtype=tf.int64),\n",
    "    tf.constant(10, dtype=tf.int64)\n",
    ")\n",
    "print(ret)\n",
    "print(cbpe.decode(ret.numpy().tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read deploy model (you should restart kernel and go from here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tf2gpt.model import GPT\n",
    "from utils.story_util import Story,Stories\n",
    "from utils.progress_bar import ProgressBar\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import random\n",
    "from utils.gpt2_tokenizer import GPT2Tokenizer\n",
    "import tensorflow_hub as hub\n",
    "cbpe = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = hub.load('./result_weights/cpm-lm-tf2-fp16-fine4m-dungeon-format2day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gpt(tokenizer, gpt, sentence, number=1, length=20):\n",
    "    inputs = tf.constant([tokenizer.encode(sentence)] * number, dtype=tf.int64)\n",
    "    length = tf.constant(length, dtype=tf.int64)\n",
    "    ret = gpt.signatures['serving_default'](inp=inputs, length=length)['output_0']\n",
    "    return [\n",
    "        tokenizer.decode(s).replace(' ', '')\n",
    "        for s in ret.numpy()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.676 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个公司老总,你事业正在上升期,你娶了一个美丽的妻子\n",
      ">你走进你的家门\n",
      "玛丽,埃莉诺,孩子和他们的朋友走近你。当他们到达的时候,他们开始哄笑起来。“爸爸,妈妈,我们能和你玩吗?”玛丽发出了命令。你点点头,说:“当然,好的。当我和你们一起去骑马时,我想我会用这只手把它们从鞍子上拿开。”\n",
      ">你骑马去看\n",
      "当其他的孩子都加入战斗,你骑着你的骏马穿过城镇,将两个时代分开。当\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "q = f'''你是一个公司老总，你事业正在上升期，你娶了一个美丽的妻子\n",
    "> 你走进你的家门\n",
    "'''\n",
    "ret = sample_gpt(cbpe, gpt, q, 1, 150)\n",
    "for x in ret:\n",
    "    print(x)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Story():\n",
    "    def __init__(self,beginning,story_max_len=150,context_len=6):\n",
    "        self.story = [beginning]\n",
    "        self.story_max_len = story_max_len\n",
    "        self.context_len = context_len\n",
    "    \n",
    "    def action(self,action):\n",
    "        action_str = \"> 你\" + action\n",
    "        self.story.append(action_str)\n",
    "        q = \"\\n\".join(self.story[-self.context_len:])\n",
    "        response = sample_gpt(cbpe, gpt, q, 1, 150)[0][len(q):]\n",
    "        self.story.append(response)\n",
    "        \n",
    "    def interactive(self):\n",
    "        print(\"\\n\".join(self.story))\n",
    "        while True:\n",
    "            action = input(\"> 你\")\n",
    "            self.action(action)\n",
    "            print()\n",
    "            print(self.story[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = f'''你在树林里冒险，指不定会从哪里蹦出来一些奇怪的东西，你握紧手上的手枪，希望这次冒险能够找到一些值钱的东西'''\n",
    "story = Story(begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你在树林里冒险，指不定会从哪里蹦出来一些奇怪的东西，你握紧手上的手枪，希望这次冒险能够找到一些值钱的东西\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 向树林深处走去\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "又走了好一会儿,天空中闪烁着明亮的光,可以让您看到厚厚的森林。森林真的很茂密,简直盖过了一座小山。光线从树叶的缝隙中穿过,洒在到处都是苔藓的地面上。一块足够小的地方可以放下你的任何东西,这里只有一个入口。\n",
      ">你向前方跑去\n",
      "你又向前跑了一段路,接着就穿过了一排排小房子,进入到一个闪烁着明亮光的新绿之中。森林的最北端是一片绿色的山丘\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 走上绿色的山丘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!这是一个尽头!你超越了上方的树枝,开始迈开两条腿走。远处的太阳照在地面上,你们能看到下面的大地和蓝天。你带着一股干净的气息站在他们面前,在它温暖的空气中嗅着它的芳香。你四处走动,寻找一些舒服的地方,然后试图将注意力集中在工作上。你在这里感觉就像是重新开始,并没有多么不适合那个在遭遇那些黑暗森林居民的泥土之前一直被囚禁的地方。你决定冒险进入他们的地区,在他\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 寻找黑暗森林居民\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "你的眼睛盯着周围的森林,确保它荫凉、新鲜、没有被害虫吞噬的东西。当你开始四处漫步时,冰冷的光线洒在你的身上,你感觉像是在走向坟墓。突然,你看到了一个黑暗的阴影,坐在周围的树林阴影的一侧。“那是什么,小丑?我看不清楚,但它看起来就像是我生命中最重要的东西。你那双毛茸茸的身体就像是流着口水的白色犀牛。......”你听到它的声音,\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 问他是谁\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "说它是什么,小丑?”你朝它靠近,怀疑地收起了刚才在本能下可能看到的手臂和腿。“它是一个女人,”你说,举起了一把匕首,“她在这里,你看看。我不想这么做,因为它看起来太令人惊讶了。我不应该被她占据时间,更不想做一个猎物。”你听到她咯咯的笑声,当你凝视她时,她将笑声投向你。你张开嘴巴,慢慢地张开\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 问她是什么人\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "那一刻真的是在搞笑,在你以一种痛苦的好奇心看着她。然后你开始享受其中的乐趣,转向马,它已经开始跑向你。几分钟后,尽管你觉得有点累,你相信小丑真的存在。马身细长,可是每个部分都小得可怜,边跑边拉,旋转,骑,边撒吃的。你骑着它飞驰而去,飞向远方。你回头看了看,小丑仍然在他的阴影中闲逛,眼睛在悄悄望着什么。你环顾四\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 ok你到底是谁\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "知不觉已经跨过了地雷区,越过了叛徒。当你猛踩加速时,小丑举起了剑。你将匕首刺入他。一阵风,有微风和尘土吹开了他,时间似乎静止了。你们在一棵大树下面,“蒂德,我来消灭你。我在树林里长大,是个德鲁伊,这是我的故事。我是人类,是个性感玛丽。我只想知道自己是谁,想寻找自己的生活。但你知道,这并不重要。\n"
     ]
    }
   ],
   "source": [
    "story.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
