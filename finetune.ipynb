{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tf2gpt.model import GPT\n",
    "from utils.story_util import Story,Stories\n",
    "from utils.progress_bar import ProgressBar\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16 <dtype: 'float16'> True\n"
     ]
    }
   ],
   "source": [
    "#with mirrored_strategy.scope():\n",
    "gpt = GPT(\n",
    "    vocab_size=30_000,\n",
    "    layer_size=32,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=2560,\n",
    "    num_attention_heads=32,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0,\n",
    "    train_size=499\n",
    ")\n",
    "gpt.load_weights('./gpt_weight_pretrain/weight_fp16')\n",
    "\n",
    "#input_x = tf.keras.layers.Input((499,), dtype=tf.int32)\n",
    "#outputs = gpt_origin(input_x)\n",
    "\n",
    "#gpt = tf.keras.Model(inputs=input_x, outputs=outputs)\n",
    "#gpt = multi_gpu_model(gpt, gpus=8)\n",
    "\n",
    "print(tf.keras.backend.floatx(), tf.float16, tf.keras.backend.floatx() == tf.float16)\n",
    "if tf.keras.backend.floatx() == tf.float16:\n",
    "    for x in gpt.weights:\n",
    "        assert x.dtype == tf.float16\n",
    "\n",
    "\n",
    "gpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gpt2_tokenizer import GPT2Tokenizer\n",
    "cbpe = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.599 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[837, 259, 497, 57, 8, 237]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气还行')\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "气 不错 不错行 \n",
      "------------------------------\n",
      "气 不错 不错行 ,\n",
      "------------------------------\n",
      "气 不错 不错行 , 我\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着 去\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着 去 看看\n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着 去 看看 \n",
      "------------------------------\n",
      "气 不错 不错行 , 我 就 想着 去 看看 ,\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_basic_logic():\n",
    "    ids = cbpe.encode('今天天气还行')\n",
    "    #print(ids)\n",
    "    print(\"+\" * 20)\n",
    "    for i in range(10):\n",
    "        output = gpt(tf.constant([ids]))\n",
    "        #print(output[0].shape)\n",
    "        nid = np.argmax(output[0, -1])\n",
    "        ids += [nid]\n",
    "        #print(i, cbpe.decode(ids))\n",
    "        #print(np.argmax(output[0],axis=-1))\n",
    "        print(cbpe.decode(np.argmax(output[0],axis=-1)))\n",
    "        print('-' * 30)\n",
    "test_basic_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(learning_rate=6e-4,\n",
    "                      warmup_steps=20_0000,\n",
    "                      decay_steps=200_0000,\n",
    "                      alpha=0.0):\n",
    "    def decayed_learning_rate(step=1):\n",
    "        if step <= warmup_steps:\n",
    "            mult = step / float(warmup_steps)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (decay_steps - warmup_steps)\n",
    "            mult = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "            mult = max(0.1, mult)\n",
    "        return learning_rate * mult\n",
    "    return decayed_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = Stories(\"./labeled_data/advanture_translated/processed_translated_story.txt\").stories\n",
    "#stories = stories[:50]\n",
    "data_folder = \"./labeled_data/\"\n",
    "txt_files = [(data_folder + i) for i in os.listdir(data_folder) if \"txt\" in i]\n",
    "#stories = stories[:10]\n",
    "stories = [Story(\"\",\"\").from_file(i) for i in txt_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stories = Stories(\"./labeled_data/advanture_translated/processed_translated_story_valid.txt\").stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 35)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stories),len(valid_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def data_generator(stories, batch_size=4,sample_len=200,inf=False):\n",
    "    while True:\n",
    "        batch_data = []\n",
    "        tmp_stories = copy.copy(stories)\n",
    "        random.shuffle(tmp_stories)\n",
    "        for i,one_story in enumerate(tmp_stories):\n",
    "            story_content = one_story.to_dungeon_format()\n",
    "            story_content = story_content.replace(\"<start>\\n\",\"\")\n",
    "            story_content = story_content.replace(\"\\n<end>\",\"\")\n",
    "            story_content = story_content.replace(\"\\n<end>\",\"\")\n",
    "            story_content = story_content.replace(\" \",\"\")\n",
    "            ids = cbpe.encode(story_content)\n",
    "            while ids:\n",
    "                sample = ids[:sample_len]\n",
    "                ids = ids[sample_len:]\n",
    "                if len(sample) < sample_len:\n",
    "                    sample += [0 for i in range((sample_len - len(sample)))]\n",
    "                batch_data.append(sample)\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    yield np.asarray(batch_data)\n",
    "                    batch_data = []\n",
    "        if not inf:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = data_generator(valid_stories,sample_len=500,batch_size=2,inf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 500)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_gen.__next__().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "tl: 2.28515625 vl: 100 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11740 ss\n",
      "Epoch 2\n",
      "tl: 2.333984375 vl: 2.90625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11427 s\n",
      "Epoch 3\n",
      "tl: 2.396484375 vl: 2.703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11146 s\n",
      "Epoch 4\n",
      "tl: 2.28515625 vl: 2.703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10929 ss\n",
      "Epoch 5\n",
      "tl: 2.333984375 vl: 2.8125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11137 s\n",
      "Epoch 6\n",
      "tl: 2.171875 vl: 2.744140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11304 s\n",
      "Epoch 7\n",
      "tl: 2.33203125 vl: 2.744140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10613 ss\n",
      "Epoch 8\n",
      "tl: 2.39453125 vl: 2.91796875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11127 ss\n",
      "Epoch 9\n",
      "tl: 2.283203125 vl: 2.990234375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11219 s\n",
      "Epoch 10\n",
      "tl: 2.390625 vl: 2.87890625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11173 s52 s\n",
      "Epoch 11\n",
      "tl: 2.431640625 vl: 2.87890625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10615 s\n",
      "Epoch 12\n",
      "tl: 2.353515625 vl: 2.90625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11165 s2 s\n",
      "Epoch 13\n",
      "tl: 2.3125 vl: 2.978515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11207 s306 s\n",
      "Epoch 14\n",
      "tl: 2.326171875 vl: 2.978515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10745 s\n",
      "Epoch 15\n",
      "tl: 2.16796875 vl: 2.8828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11097 ss\n",
      "Epoch 16\n",
      "tl: 2.166015625 vl: 2.703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11241 ss\n",
      "Epoch 17\n",
      "tl: 2.423828125 vl: 2.703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10729 s\n",
      "Epoch 18\n",
      "tl: 2.2734375 vl: 2.666015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11089 sss\n",
      "Epoch 19\n",
      "tl: 2.1640625 vl: 2.74609375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11329 s ss\n",
      "Epoch 20\n",
      "tl: 2.4296875 vl: 2.59765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11116 s s\n",
      "Epoch 21\n",
      "tl: 2.26953125 vl: 2.59765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10773 ss\n",
      "Epoch 22\n",
      "tl: 2.26953125 vl: 2.896484375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11318 ss\n",
      "Epoch 23\n",
      "tl: 2.267578125 vl: 2.548828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11252 s\n",
      "Epoch 24\n",
      "tl: 2.267578125 vl: 2.548828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10646 s\n",
      "Epoch 25\n",
      "tl: 2.37109375 vl: 2.435546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11215 ss\n",
      "Epoch 26\n",
      "tl: 2.15625 vl: 2.708984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11297 s89 s\n",
      "Epoch 27\n",
      "tl: 2.263671875 vl: 2.708984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10683 s\n",
      "Epoch 28\n",
      "tl: 2.33203125 vl: 2.7890625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11270 ss\n",
      "Epoch 29\n",
      "tl: 2.259765625 vl: 2.603515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11223 s\n",
      "Epoch 30\n",
      "tl: 2.400390625 vl: 2.54296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11301 ss\n",
      "Epoch 31\n",
      "tl: 2.150390625 vl: 2.54296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10699 s\n",
      "Epoch 32\n",
      "tl: 2.1484375 vl: 2.912109375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11202 sss\n",
      "Epoch 33\n",
      "tl: 2.1484375 vl: 3.109375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11262 ss41 s\n",
      "Epoch 34\n",
      "tl: 2.41015625 vl: 3.109375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10704 ss\n",
      "Epoch 35\n",
      "tl: 2.302734375 vl: 2.703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11214 s\n",
      "Epoch 36\n",
      "tl: 2.35546875 vl: 2.685546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11325 ss\n",
      "Epoch 37\n",
      "tl: 2.142578125 vl: 2.685546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10690 s\n",
      "Epoch 38\n",
      "tl: 2.142578125 vl: 2.787109375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11324 s\n",
      "Epoch 39\n",
      "tl: 2.244140625 vl: 2.970703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11179 s\n",
      "Epoch 40\n",
      "tl: 2.294921875 vl: 2.7890625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11303 s s\n",
      "Epoch 41\n",
      "tl: 2.310546875 vl: 2.7890625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10576 s\n",
      "Epoch 42\n",
      "tl: 2.400390625 vl: 2.666015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11246 s\n",
      "Epoch 43\n",
      "tl: 2.240234375 vl: 2.666015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11243 s\n",
      "Epoch 44\n",
      "tl: 2.1328125 vl: 2.666015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10762 s s\n",
      "Epoch 45\n",
      "tl: 2.39453125 vl: 2.748046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11252 ss\n",
      "Epoch 46\n",
      "tl: 2.130859375 vl: 2.873046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11057 s\n",
      "Epoch 47\n",
      "tl: 2.12890625 vl: 2.873046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10597 ss\n",
      "Epoch 48\n",
      "tl: 2.259765625 vl: 2.64453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11137 s\n",
      "Epoch 49\n",
      "tl: 2.23046875 vl: 2.697265625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11206 ss\n",
      "Epoch 50\n",
      "tl: 2.125 vl: 2.66015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11324 s10761 s\n",
      "Epoch 51\n",
      "tl: 2.279296875 vl: 2.66015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10637 s\n",
      "Epoch 52\n",
      "tl: 2.123046875 vl: 2.498046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11139 s\n",
      "Epoch 53\n",
      "tl: 2.3828125 vl: 2.73046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11200 s ss\n",
      "Epoch 54\n",
      "tl: 2.380859375 vl: 2.73046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10741 s\n",
      "Epoch 55\n",
      "tl: 2.220703125 vl: 2.83203125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11373 s\n",
      "Epoch 56\n",
      "tl: 2.2890625 vl: 3.013671875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11096 s s\n",
      "Epoch 57\n",
      "tl: 2.287109375 vl: 3.013671875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10654 s\n",
      "Epoch 58\n",
      "tl: 2.2421875 vl: 2.86328125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11156 sss\n",
      "Epoch 59\n",
      "tl: 2.35546875 vl: 2.810546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11215 ss\n",
      "Epoch 60\n",
      "tl: 2.240234375 vl: 2.798828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11283 s\n",
      "Epoch 61\n",
      "tl: 2.26171875 vl: 2.798828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10705 ss\n",
      "Epoch 62\n",
      "tl: 2.107421875 vl: 2.908203125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11268 s\n",
      "Epoch 63\n",
      "tl: 2.107421875 vl: 2.8828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11103 s s\n",
      "Epoch 64\n",
      "tl: 2.2578125 vl: 2.8828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10623 s s\n",
      "Epoch 65\n",
      "tl: 2.103515625 vl: 2.376953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11252 s\n",
      "Epoch 66\n",
      "tl: 2.1015625 vl: 2.634765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11236 s s\n",
      "Epoch 67\n",
      "tl: 2.203125 vl: 2.634765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10677 s ss\n",
      "Epoch 68\n",
      "tl: 2.306640625 vl: 2.6875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11213 s\n",
      "Epoch 69\n",
      "tl: 2.19921875 vl: 3.08984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11191 ss\n",
      "Epoch 70\n",
      "tl: 2.265625 vl: 2.724609375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11302 sss\n",
      "Epoch 71\n",
      "tl: 2.35546875 vl: 2.724609375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10591 ss\n",
      "Epoch 72\n",
      "tl: 2.1953125 vl: 2.833984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11232 sss\n",
      "Epoch 73\n",
      "tl: 2.298828125 vl: 2.81640625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11125 ss\n",
      "Epoch 74\n",
      "tl: 2.193359375 vl: 2.81640625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10749 s\n",
      "Epoch 75\n",
      "tl: 2.08984375 vl: 2.73828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11074 ss\n",
      "Epoch 76\n",
      "tl: 2.087890625 vl: 2.958984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11155 s\n",
      "Epoch 77\n",
      "tl: 2.291015625 vl: 2.958984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10815 s\n",
      "Epoch 78\n",
      "tl: 2.20703125 vl: 2.71875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11190 ss\n",
      "Epoch 79\n",
      "tl: 2.234375 vl: 2.75390625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11331 s2 s\n",
      "Epoch 80\n",
      "tl: 2.18359375 vl: 3.099609375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11188 s\n",
      "Epoch 81\n",
      "tl: 2.28515625 vl: 3.099609375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10772 ss\n",
      "Epoch 82\n",
      "tl: 2.201171875 vl: 2.68359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11261 ss\n",
      "Epoch 83\n",
      "tl: 2.28125 vl: 3.45703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11141 ss ss\n",
      "Epoch 84\n",
      "tl: 2.224609375 vl: 3.45703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10778 s\n",
      "Epoch 85\n",
      "tl: 2.240234375 vl: 2.533203125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11124 s\n",
      "Epoch 86\n",
      "tl: 2.240234375 vl: 2.39453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11209 ss\n",
      "Epoch 87\n",
      "tl: 2.173828125 vl: 2.39453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10780 s\n",
      "Epoch 88\n",
      "tl: 2.30859375 vl: 2.609375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11177 ss\n",
      "Epoch 89\n",
      "tl: 2.06640625 vl: 2.44140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11326 ss\n",
      "Epoch 90\n",
      "tl: 2.06640625 vl: 2.869140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11358 s\n",
      "Epoch 91\n",
      "tl: 2.064453125 vl: 2.869140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10936 s\n",
      "Epoch 92\n",
      "tl: 2.32421875 vl: 2.802734375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11086 ss\n",
      "Epoch 93\n",
      "tl: 2.060546875 vl: 2.78125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11139 s65 s\n",
      "Epoch 94\n",
      "tl: 2.05859375 vl: 2.78125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10974 ss\n",
      "Epoch 95\n",
      "tl: 2.162109375 vl: 2.703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11754 s\n",
      "Epoch 96\n",
      "tl: 2.16015625 vl: 2.900390625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11309 ss\n",
      "Epoch 97\n",
      "tl: 2.052734375 vl: 2.900390625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10696 s\n",
      "Epoch 98\n",
      "tl: 2.173828125 vl: 2.83984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11132 s\n",
      "Epoch 99\n",
      "tl: 2.15625 vl: 2.755859375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11328 s s s\n",
      "Epoch 100\n",
      "tl: 2.048828125 vl: 2.353515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11129 s\n",
      "Epoch 101\n",
      "tl: 2.287109375 vl: 2.353515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10806 s\n",
      "Epoch 102\n",
      "tl: 2.150390625 vl: 2.828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11264 s8 s\n",
      "Epoch 103\n",
      "tl: 2.216796875 vl: 2.91015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11041 s\n",
      "Epoch 104\n",
      "tl: 2.146484375 vl: 2.91015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10705 s\n",
      "Epoch 105\n",
      "tl: 2.0390625 vl: 2.701171875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11151 sss\n",
      "Epoch 106\n",
      "tl: 2.14453125 vl: 2.818359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11312 ss\n",
      "Epoch 107\n",
      "tl: 2.275390625 vl: 2.818359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10584 s\n",
      "Epoch 108\n",
      "tl: 2.2109375 vl: 2.421875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11422 ss\n",
      "Epoch 109\n",
      "tl: 2.2421875 vl: 2.2265625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11266 s s\n",
      "Epoch 110\n",
      "tl: 2.03125 vl: 2.427734375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11210 s s\n",
      "Epoch 111\n",
      "tl: 2.029296875 vl: 2.427734375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10807 s\n",
      "Epoch 112\n",
      "tl: 2.203125 vl: 2.22265625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11339 ss ss\n",
      "Epoch 113\n",
      "tl: 2.1328125 vl: 1.4892578125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11349 s\n",
      "Epoch 114\n",
      "tl: 2.025390625 vl: 1.4892578125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10704 s\n",
      "Epoch 115\n",
      "tl: 2.2890625 vl: 1.3134765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11401 sss\n",
      "Epoch 116\n",
      "tl: 2.126953125 vl: 1.333984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11215 ss\n",
      "Epoch 117\n",
      "tl: 2.228515625 vl: 1.333984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10728 s\n",
      "Epoch 118\n",
      "tl: 2.28515625 vl: 2.4140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11210 ss\n",
      "Epoch 119\n",
      "tl: 2.017578125 vl: 2.81640625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11421 s\n",
      "Epoch 120\n",
      "tl: 2.017578125 vl: 2.251953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11163 s\n",
      "Epoch 121\n",
      "tl: 2.15625 vl: 2.251953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10756 s67 s\n",
      "Epoch 122\n",
      "tl: 2.27734375 vl: 2.392578125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11248 ss\n",
      "Epoch 123\n",
      "tl: 2.11328125 vl: 2.615234375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11253 ss\n",
      "Epoch 124\n",
      "tl: 2.126953125 vl: 2.615234375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10720 s\n",
      "Epoch 125\n",
      "tl: 2.111328125 vl: 2.91796875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11659 s\n",
      "Epoch 126\n",
      "tl: 2.2109375 vl: 2.466796875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11366 s s\n",
      "Epoch 127\n",
      "tl: 2.005859375 vl: 2.466796875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10684 s\n",
      "Epoch 128\n",
      "tl: 2.234375 vl: 2.884765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11560 s6 s\n",
      "Epoch 129\n",
      "tl: 2.232421875 vl: 2.900390625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11470 s\n",
      "Epoch 130\n",
      "tl: 2.23046875 vl: 2.91015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11638 sss\n",
      "Epoch 131\n",
      "tl: 2.26171875 vl: 2.91015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10772 sss\n",
      "Epoch 132\n",
      "tl: 1.9970703125 vl: 2.673828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11129 s\n",
      "Epoch 133\n",
      "tl: 1.9951171875 vl: 2.751953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11313 s\n",
      "Epoch 134\n",
      "tl: 2.087890625 vl: 2.751953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10736 s\n",
      "Epoch 135\n",
      "tl: 2.25390625 vl: 2.919921875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11216 ss\n",
      "Epoch 136\n",
      "tl: 1.990234375 vl: 2.9765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11202 sss\n",
      "Epoch 137\n",
      "tl: 2.154296875 vl: 2.9765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10606 ss\n",
      "Epoch 138\n",
      "tl: 2.080078125 vl: 2.521484375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11317 ss\n",
      "Epoch 139\n",
      "tl: 2.123046875 vl: 3.05078125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11157 s\n",
      "Epoch 140\n",
      "tl: 2.076171875 vl: 2.884765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11178 s\n",
      "Epoch 141\n",
      "tl: 1.98046875 vl: 2.884765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10594 ss\n",
      "Epoch 142\n",
      "tl: 2.072265625 vl: 2.798828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11205 ss\n",
      "Epoch 143\n",
      "tl: 2.146484375 vl: 2.755859375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11206 s\n",
      "Epoch 144\n",
      "tl: 2.068359375 vl: 2.755859375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10576 s\n",
      "Epoch 145\n",
      "tl: 2.19140625 vl: 2.845703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11191 ss\n",
      "Epoch 146\n",
      "tl: 2.16796875 vl: 2.876953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11201 s s\n",
      "Epoch 147\n",
      "tl: 2.060546875 vl: 2.876953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10800 ss\n",
      "Epoch 148\n",
      "tl: 1.9658203125 vl: 2.900390625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11289 s\n",
      "Epoch 149\n",
      "tl: 2.177734375 vl: 2.68359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11834 ss\n",
      "Epoch 150\n",
      "tl: 2.095703125 vl: 2.861328125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11792 s\n",
      "Epoch 151\n",
      "tl: 2.22265625 vl: 2.861328125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10715 s s\n",
      "Epoch 152\n",
      "tl: 2.064453125 vl: 2.794921875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11866 s\n",
      "Epoch 153\n",
      "tl: 1.9541015625 vl: 2.38671875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11173 s\n",
      "Epoch 154\n",
      "tl: 2.05859375 vl: 2.38671875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10664 s s\n",
      "Epoch 155\n",
      "tl: 1.94921875 vl: 2.802734375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11318 ss\n",
      "Epoch 156\n",
      "tl: 1.947265625 vl: 2.84375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11100 s97 s\n",
      "Epoch 157\n",
      "tl: 2.048828125 vl: 2.84375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11174 s\n",
      "Epoch 158\n",
      "tl: 2.03125 vl: 3.04296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11283 s62 s\n",
      "Epoch 159\n",
      "tl: 1.9404296875 vl: 2.76171875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11258 s\n",
      "Epoch 160\n",
      "tl: 2.0703125 vl: 2.70703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11279 s6 s\n",
      "Epoch 161\n",
      "tl: 2.189453125 vl: 2.70703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10596 s\n",
      "Epoch 162\n",
      "tl: 2.021484375 vl: 3.0 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11204 ss0300 s\n",
      "Epoch 163\n",
      "tl: 2.05859375 vl: 2.509765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11212 ss\n",
      "Epoch 164\n",
      "tl: 2.142578125 vl: 2.509765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10724 ss\n",
      "Epoch 165\n",
      "tl: 2.1796875 vl: 2.837890625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11093 s s\n",
      "Epoch 166\n",
      "tl: 1.9169921875 vl: 2.85546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11152 s\n",
      "Epoch 167\n",
      "tl: 2.169921875 vl: 2.85546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10739 ss\n",
      "Epoch 168\n",
      "tl: 2.0 vl: 2.962890625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11193 s:11268 s\n",
      "Epoch 169\n",
      "tl: 2.013671875 vl: 2.9140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11225 sss\n",
      "Epoch 170\n",
      "tl: 1.9951171875 vl: 2.70703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11151 s\n",
      "Epoch 171\n",
      "tl: 2.1171875 vl: 2.70703125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10680 s s\n",
      "Epoch 172\n",
      "tl: 2.01953125 vl: 2.423828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11243 ss\n",
      "Epoch 173\n",
      "tl: 1.89453125 vl: 2.9609375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11273 sss s\n",
      "Epoch 174\n",
      "tl: 1.890625 vl: 2.9609375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10723 s sss\n",
      "Epoch 175\n",
      "tl: 1.9765625 vl: 2.671875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11154 s ss\n",
      "Epoch 176\n",
      "tl: 2.005859375 vl: 2.9765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11605 ss\n",
      "Epoch 177\n",
      "tl: 2.04296875 vl: 2.9765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10734 s s\n",
      "Epoch 178\n",
      "tl: 1.96875 vl: 2.732421875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11120 s70 ss\n",
      "Epoch 179\n",
      "tl: 1.873046875 vl: 2.853515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11323 ss\n",
      "Epoch 180\n",
      "tl: 1.8701171875 vl: 2.728515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11204 s\n",
      "Epoch 181\n",
      "tl: 2.111328125 vl: 2.728515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11135 ss\n",
      "Epoch 182\n",
      "tl: 1.8623046875 vl: 2.71875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11224 s ss\n",
      "Epoch 183\n",
      "tl: 1.9501953125 vl: 2.84375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11249 s\n",
      "Epoch 184\n",
      "tl: 1.9599609375 vl: 2.84375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10973 s\n",
      "Epoch 185\n",
      "tl: 1.8486328125 vl: 3.07421875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11143 s\n",
      "Epoch 186\n",
      "tl: 1.8447265625 vl: 2.994140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11186 s\n",
      "Epoch 187\n",
      "tl: 1.93359375 vl: 2.994140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10708 ss\n",
      "Epoch 188\n",
      "tl: 2.04296875 vl: 2.857421875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11132 s s\n",
      "Epoch 189\n",
      "tl: 2.06640625 vl: 3.091796875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11281 sss\n",
      "Epoch 190\n",
      "tl: 2.03515625 vl: 3.04296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11627 s3 s\n",
      "Epoch 191\n",
      "tl: 1.9931640625 vl: 3.04296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10857 s\n",
      "Epoch 192\n",
      "tl: 1.814453125 vl: 2.8203125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11261 sss\n",
      "Epoch 193\n",
      "tl: 2.048828125 vl: 3.001953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11286 ss\n",
      "Epoch 194\n",
      "tl: 1.896484375 vl: 3.001953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10696 s\n",
      "Epoch 195\n",
      "tl: 1.916015625 vl: 2.658203125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11095 ss\n",
      "Epoch 196\n",
      "tl: 1.7978515625 vl: 2.64453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11275 s\n",
      "Epoch 197\n",
      "tl: 1.8818359375 vl: 2.64453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10562 s\n",
      "Epoch 198\n",
      "tl: 1.8798828125 vl: 2.13671875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11168 s\n",
      "Epoch 199\n",
      "tl: 1.880859375 vl: 2.68359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11338 s\n",
      "Epoch 200\n",
      "tl: 1.8759765625 vl: 2.9375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11321 s82 s\n",
      "Epoch 201\n",
      "tl: 1.818359375 vl: 2.9375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10722 ss\n",
      "Epoch 202\n",
      "tl: 1.994140625 vl: 2.892578125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11216 ss\n",
      "Epoch 203\n",
      "tl: 1.96484375 vl: 2.64453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11351 s s\n",
      "Epoch 204\n",
      "tl: 1.861328125 vl: 2.64453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10640 ss\n",
      "Epoch 205\n",
      "tl: 1.7490234375 vl: 3.16015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11446 s\n",
      "Epoch 206\n",
      "tl: 1.744140625 vl: 3.0625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11238 s26 ss\n",
      "Epoch 207\n",
      "tl: 1.95703125 vl: 3.0625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10623 s s\n",
      "Epoch 208\n",
      "tl: 1.833984375 vl: 4.02734375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11436 ss\n",
      "Epoch 209\n",
      "tl: 1.94140625 vl: 2.359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11248 s sss\n",
      "Epoch 210\n",
      "tl: 1.8212890625 vl: 1.662109375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11250 s\n",
      "Epoch 211\n",
      "tl: 1.8076171875 vl: 1.662109375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10684 s\n",
      "Epoch 212\n",
      "tl: 1.802734375 vl: 1.466796875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11123 ss\n",
      "Epoch 213\n",
      "tl: 1.7939453125 vl: 1.46875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11206 ss ss\n",
      "Epoch 214\n",
      "tl: 1.701171875 vl: 1.46875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10654 ss\n",
      "Epoch 215\n",
      "tl: 1.875 vl: 2.59375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11428 s16 s2 s\n",
      "Epoch 216\n",
      "tl: 1.6904296875 vl: 2.80078125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11136 s\n",
      "Epoch 217\n",
      "tl: 1.7841796875 vl: 2.80078125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10851 s\n",
      "Epoch 218\n",
      "tl: 1.7548828125 vl: 2.79296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11503 s\n",
      "Epoch 219\n",
      "tl: 1.861328125 vl: 2.859375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11354 s s\n",
      "Epoch 220\n",
      "tl: 1.6630859375 vl: 3.123046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11239 s\n",
      "Epoch 221\n",
      "tl: 1.78125 vl: 3.123046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10706 s680 s\n",
      "Epoch 222\n",
      "tl: 1.7861328125 vl: 3.06640625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11327 ss\n",
      "Epoch 223\n",
      "tl: 1.7294921875 vl: 3.04296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11280 s\n",
      "Epoch 224\n",
      "tl: 1.6376953125 vl: 3.04296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10804 s\n",
      "Epoch 225\n",
      "tl: 1.7177734375 vl: 3.013671875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11296 s\n",
      "Epoch 226\n",
      "tl: 1.7138671875 vl: 2.9375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11371 s484 s\n",
      "Epoch 227\n",
      "tl: 1.7451171875 vl: 2.9375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10691 s\n",
      "Epoch 228\n",
      "tl: 1.69921875 vl: 2.73046875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11137 s s\n",
      "Epoch 229\n",
      "tl: 1.771484375 vl: 2.61328125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11143 ss\n",
      "Epoch 230\n",
      "tl: 1.6845703125 vl: 2.806640625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11358 s\n",
      "Epoch 231\n",
      "tl: 1.6767578125 vl: 2.806640625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10686 s\n",
      "Epoch 232\n",
      "tl: 1.6708984375 vl: 2.5390625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11225 s s\n",
      "Epoch 233\n",
      "tl: 1.693359375 vl: 2.439453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11209 s\n",
      "Epoch 234\n",
      "tl: 1.6474609375 vl: 2.439453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10655 s\n",
      "Epoch 235\n",
      "tl: 1.6865234375 vl: 2.759765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11309 s\n",
      "Epoch 236\n",
      "tl: 1.634765625 vl: 2.96875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11266 ss99 s\n",
      "Epoch 237\n",
      "tl: 1.6962890625 vl: 2.96875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10778 s\n",
      "Epoch 238\n",
      "tl: 1.7255859375 vl: 2.912109375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11161 s\n",
      "Epoch 239\n",
      "tl: 1.599609375 vl: 3.1953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11141 sss\n",
      "Epoch 240\n",
      "tl: 1.646484375 vl: 2.79296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11280 s\n",
      "Epoch 241\n",
      "tl: 1.658203125 vl: 2.79296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10659 ss\n",
      "Epoch 242\n",
      "tl: 1.61328125 vl: 2.9375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11336 ss277 s\n",
      "Epoch 243\n",
      "tl: 1.564453125 vl: 3.068359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11259 ss\n",
      "Epoch 244\n",
      "tl: 1.4873046875 vl: 3.068359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10708 s\n",
      "Epoch 245\n",
      "tl: 1.5908203125 vl: 3.1484375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11135 s\n",
      "Epoch 246\n",
      "tl: 1.5380859375 vl: 2.833984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11193 s\n",
      "Epoch 247\n",
      "tl: 1.5224609375 vl: 2.833984375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10732 s\n",
      "Epoch 248\n",
      "tl: 1.583984375 vl: 3.16015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11233 ss\n",
      "Epoch 249\n",
      "tl: 1.498046875 vl: 3.3359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11261 sss\n",
      "Epoch 250\n",
      "tl: 1.412109375 vl: 3.107421875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11230 s\n",
      "Epoch 251\n",
      "tl: 1.4951171875 vl: 3.107421875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10834 s\n",
      "Epoch 252\n",
      "tl: 1.482421875 vl: 3.03125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11198 ss3 s\n",
      "Epoch 253\n",
      "tl: 1.37109375 vl: 3.080078125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11165 ss\n",
      "Epoch 254\n",
      "tl: 1.5546875 vl: 3.080078125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10808 s8 s\n",
      "Epoch 255\n",
      "tl: 1.361328125 vl: 2.994140625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11226 ss\n",
      "Epoch 256\n",
      "tl: 1.419921875 vl: 3.171875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11080 ss ss\n",
      "Epoch 257\n",
      "tl: 1.4677734375 vl: 3.171875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10746 s\n",
      "Epoch 258\n",
      "tl: 1.4296875 vl: 3.076171875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11251 s5 s\n",
      "Epoch 259\n",
      "tl: 1.3798828125 vl: 2.837890625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11429 s\n",
      "Epoch 260\n",
      "tl: 1.4345703125 vl: 3.53515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11325 ss\n",
      "Epoch 261\n",
      "tl: 1.3896484375 vl: 3.53515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10841 s\n",
      "Epoch 262\n",
      "tl: 1.4462890625 vl: 3.35546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11166 s\n",
      "Epoch 263\n",
      "tl: 1.2421875 vl: 3.1484375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11246 ss9 s\n",
      "Epoch 264\n",
      "tl: 1.220703125 vl: 3.1484375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10735 ss\n",
      "Epoch 265\n",
      "tl: 1.4404296875 vl: 3.1953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11186 s\n",
      "Epoch 266\n",
      "tl: 1.322265625 vl: 3.353515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11462 ss\n",
      "Epoch 267\n",
      "tl: 1.162109375 vl: 3.353515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10543 s\n",
      "Epoch 268\n",
      "tl: 1.37109375 vl: 3.4296875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11193 s s\n",
      "Epoch 269\n",
      "tl: 1.3408203125 vl: 3.5546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11256 s\n",
      "Epoch 270\n",
      "tl: 1.1005859375 vl: 3.169921875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11307 s\n",
      "Epoch 271\n",
      "tl: 1.1904296875 vl: 3.169921875 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10924 s\n",
      "Epoch 272\n",
      "tl: 1.220703125 vl: 3.05859375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11198 sss\n",
      "Epoch 273\n",
      "tl: 1.2177734375 vl: 3.130859375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11122 s\n",
      "Epoch 274\n",
      "tl: 1.26171875 vl: 3.130859375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10790 s s\n",
      "Epoch 275\n",
      "tl: 1.1728515625 vl: 3.064453125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11452 s\n",
      "Epoch 276\n",
      "tl: 1.1669921875 vl: 3.330078125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11236 s\n",
      "Epoch 277\n",
      "tl: 1.267578125 vl: 3.330078125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10674 ss\n",
      "Epoch 278\n",
      "tl: 1.037109375 vl: 3.53515625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11356 ss\n",
      "Epoch 279\n",
      "tl: 1.1455078125 vl: 3.173828125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11254 s\n",
      "Epoch 280\n",
      "tl: 1.0126953125 vl: 3.783203125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11307 s\n",
      "Epoch 281\n",
      "tl: 0.982421875 vl: 3.783203125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10684 ss\n",
      "Epoch 282\n",
      "tl: 0.96435546875 vl: 3.541015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11299 s\n",
      "Epoch 283\n",
      "tl: 1.115234375 vl: 3.6953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11265 s s s\n",
      "Epoch 284\n",
      "tl: 1.0537109375 vl: 3.6953125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10649 s\n",
      "Epoch 285\n",
      "tl: 0.93115234375 vl: 3.36328125 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11185 s\n",
      "Epoch 286\n",
      "tl: 0.88818359375 vl: 3.443359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11209 s\n",
      "Epoch 287\n",
      "tl: 0.89599609375 vl: 3.443359375 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:10709 s\n",
      "Epoch 288\n",
      "tl: 1.0498046875 vl: 3.5546875 0.07 % [>--------------------------------------------------] 7/10000 \t used:8s eta:11404 ss\n",
      "Epoch 289\n",
      "tl: 0.83349609375 vl: 3.509765625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11124 s\n",
      "Epoch 290\n",
      "tl: 0.85009765625 vl: 3.6015625 0.07 % [>--------------------------------------------------] 7/10000 \t used:7s eta:11216 ss\n",
      "Epoch 291\n",
      "tl: 1.029296875 vl: 3.6015625 0.06 % [>--------------------------------------------------] 6/10000 \t used:6s eta:10677 s s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-71738c5f55c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         (\n\u001b[1;32m     14\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             tf.constant(x[:,1:]))\n\u001b[0m\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     17\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bd/wfmrl/projects/ChineseAiDungeon/tf2gpt/model.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             loss = tf.keras.backend.sparse_categorical_crossentropy(\n\u001b[1;32m    293\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bd/wfmrl/projects/ChineseAiDungeon/tf2gpt/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, kv_cache, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m             x, cached_kv = layer(\n\u001b[1;32m    277\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 kv_cache=kv_cache[i] if kv_cache is not None else None)\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mcached_kvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_kv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bd/wfmrl/projects/ChineseAiDungeon/tf2gpt/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, kv_cache, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# https://github.com/openai/gpt-2/blob/0574c5708b094bfa0b0f6dfe3fd284d9a045acd9/src/model.py#L123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_kv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bd/wfmrl/projects/ChineseAiDungeon/tf2gpt/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, kv_cache, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_kv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autographed_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6621\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6622\u001b[0m         \u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6623\u001b[0;31m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6625\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_restore_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mscope_name\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    809\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;34m\"\"\"Sets scope name for the current thread.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('log/finetune')\n",
    "n_iter = 0\n",
    "train_loss = 100\n",
    "val_loss = 100\n",
    "for epoch in range(400):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    pb = ProgressBar(10000)\n",
    "    pb.startjob()\n",
    "    for x in data_generator(stories,sample_len=500,batch_size=2):\n",
    "    #for x in data_generator_content(texts[508:],sample_len=400,batch_size=2):\n",
    "        n_iter += 1\n",
    "        ret = gpt.train_step(\n",
    "        (\n",
    "            tf.constant(x[:,:-1]),\n",
    "            tf.constant(x[:,1:]))\n",
    "        )\n",
    "        writer.add_scalar(\"train/loss\", ret[\"loss\"].numpy(), n_iter)\n",
    "        train_loss = ret[\"loss\"].numpy()\n",
    "        if n_iter % 10 == 0:\n",
    "            valid_x = valid_gen.__next__()\n",
    "            ret = gpt.eval_step(\n",
    "            (\n",
    "                tf.constant(valid_x[:,:-1]),\n",
    "                tf.constant(valid_x[:,1:]))\n",
    "            )\n",
    "            writer.add_scalar(\"test/loss\", ret[\"loss\"].numpy(), n_iter)\n",
    "            val_loss = ret[\"loss\"].numpy()\n",
    "        pb.info = f\"tl: {train_loss} vl: {val_loss}\"\n",
    "        pb.complete(1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories[0].to_dungeon_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "99 你 是 一个 公司老总 , 你 事业 正在 上升期 , 你 娶 了 一个 美丽 的 妻子 \n",
      " >   你 走进 你 的 家门 \n",
      " 你 走进 你 的 家门 \n",
      " 你 的 妻子 正在 厨房 忙碌 \n",
      " \n",
      " \n",
      " \n",
      " 你 的 妻子 \n",
      " 你 的 妻子 \n",
      " \n",
      " \n",
      " \n",
      " 你 的 妻子 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "q = f'''你是一个公司老总，你事业正在上升期，你娶了一个美丽的妻子\n",
    "> 你走进你的家门\n",
    "'''\n",
    "ids = cbpe.encode(q)\n",
    "#print(ids)\n",
    "#print(\"+\" * 20)\n",
    "for i in range(100):\n",
    "    output = gpt(tf.constant([ids]))\n",
    "    nid = np.argmax(output[0, -1])\n",
    "    ids += [nid]\n",
    "    print(i)\n",
    "    \n",
    "print(i,cbpe.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.save_weights('./gpt_weight_pretrain/tmp_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model (restart kernel here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tf2gpt.model import GPT\n",
    "from utils.story_util import Story,Stories\n",
    "from utils.progress_bar import ProgressBar\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import random\n",
    "tf.keras.backend.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gpt2_tokenizer import GPT2Tokenizer\n",
    "cbpe = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16 <dtype: 'float16'> True\n"
     ]
    }
   ],
   "source": [
    "#with mirrored_strategy.scope():\n",
    "gpt = GPT(\n",
    "    vocab_size=30_000,\n",
    "    layer_size=32,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=2560,\n",
    "    num_attention_heads=32,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0\n",
    ")\n",
    "gpt.load_weights('./gpt_weight_pretrain/tmp_weight')\n",
    "\n",
    "#input_x = tf.keras.layers.Input((499,), dtype=tf.int32)\n",
    "#outputs = gpt_origin(input_x)\n",
    "\n",
    "#gpt = tf.keras.Model(inputs=input_x, outputs=outputs)\n",
    "#gpt = multi_gpu_model(gpt, gpus=8)\n",
    "\n",
    "print(tf.keras.backend.floatx(), tf.float16, tf.keras.backend.floatx() == tf.float16)\n",
    "if tf.keras.backend.floatx() == tf.float16:\n",
    "    for x in gpt.weights:\n",
    "        assert x.dtype == tf.float16\n",
    "\n",
    "\n",
    "gpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.576 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 今天天气 不错 \n",
      "1 今天天气 不错 ,\n",
      "2 今天天气 不错 , 你\n",
      "3 今天天气 不错 , 你 去\n",
      "4 今天天气 不错 , 你 去 把\n",
      "5 今天天气 不错 , 你 去 把 车\n",
      "6 今天天气 不错 , 你 去 把 车子\n",
      "7 今天天气 不错 , 你 去 把 车子 发动\n",
      "8 今天天气 不错 , 你 去 把 车子 发动 \n",
      "9 今天天气 不错 , 你 去 把 车子 发动 起来\n"
     ]
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气不错')\n",
    "\n",
    "for i in range(10):\n",
    "    output = gpt(tf.constant([ids]))\n",
    "    nid = np.argmax(output[0, -1])\n",
    "    ids += [nid]\n",
    "    print(i, cbpe.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gather(a, b):\n",
    "    return tf.gather(a, b, batch_dims=1)\n",
    "\n",
    "\n",
    "def top_p_sample(logits, num_samples=1, p=0.95):\n",
    "    batch_size, vocab_size = logits.shape\n",
    "    probs = tf.nn.softmax(logits, axis=-1)\n",
    "    # [batch_size, vocab_perm]\n",
    "    indices = tf.argsort(probs, direction='DESCENDING')\n",
    "    cumulative_probabilities = tf.math.cumsum(batch_gather(probs, indices), axis=-1, exclusive=False)\n",
    "\n",
    "    # find the top pth index to cut off. careful we don't want to cutoff everything!\n",
    "    # result will be [batch_size, vocab_perm]\n",
    "    p_expanded = p if isinstance(p, float) else p[:, None]\n",
    "    exclude_mask = tf.logical_not(\n",
    "        tf.logical_or(cumulative_probabilities < p_expanded, tf.range(vocab_size)[None] < 1))\n",
    "\n",
    "    # OPTION A - sample in the sorted space, then unsort.\n",
    "    logits_to_use = batch_gather(logits, indices) - tf.cast(exclude_mask, tf.float16) * 1e4\n",
    "    sample_perm = tf.random.categorical(logits=logits_to_use, num_samples=num_samples)\n",
    "    sample = batch_gather(indices, sample_perm)\n",
    "\n",
    "    return tf.cast(sample, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def serve(inputs):\n",
    "    return gpt(inputs, kv_cache=None, use_cache=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def serve_cache(inputs, kv_cache):\n",
    "    return gpt(inputs, kv_cache=kv_cache, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_concrete = serve.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\")\n",
    ")\n",
    "\n",
    "layer_size = 32\n",
    "attention_head = 32\n",
    "embedding_size = 2560\n",
    "\n",
    "serve_cache_concrete = serve_cache.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\"),\n",
    "    tf.TensorSpec(shape=[\n",
    "        layer_size, None, 2, attention_head,\n",
    "        None, embedding_size // attention_head\n",
    "    ], dtype=tf.float16, name=\"kv_cache\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = serve_concrete(\n",
    "    tf.constant([[1]], tf.int64)\n",
    ")\n",
    "r2 = serve_cache_concrete(\n",
    "    tf.constant([[1]], tf.int64),\n",
    "    r[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample(initial_inputs, length):\n",
    "    layer_size = 32\n",
    "    embedding_size = 2560\n",
    "    attention_head = 32\n",
    "\n",
    "    i = tf.constant(0, dtype=tf.int64)\n",
    "    initial_logits, kv_cache = serve(initial_inputs)\n",
    "    inputs = top_p_sample(initial_logits[:, -1, :])\n",
    "    stores = tf.concat([initial_inputs, inputs], axis=1)\n",
    "\n",
    "    def _cond(i, inputs, kv_cache, stores):\n",
    "        return i < length\n",
    "\n",
    "    def _body(i, inputs, kv_cache, stores):\n",
    "        new_logits, new_kv_cache = serve_cache(inputs, kv_cache)\n",
    "        \n",
    "        new_inputs = top_p_sample(new_logits[:, -1, :])\n",
    "        new_stores = tf.concat([stores, new_inputs], axis=-1)\n",
    "        new_kv_cache = tf.concat([\n",
    "            kv_cache,\n",
    "            new_kv_cache\n",
    "        ], axis=-2)\n",
    "        new_i = i + 1\n",
    "        return [new_i, new_inputs, new_kv_cache, new_stores]\n",
    "\n",
    "    result = tf.while_loop(\n",
    "        _cond, _body,\n",
    "        loop_vars=[i, inputs, kv_cache, stores],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape(None),\n",
    "            tf.TensorShape([None, None]),\n",
    "            tf.TensorShape([\n",
    "                layer_size, None, 2,\n",
    "                attention_head, None,\n",
    "                embedding_size // attention_head\n",
    "            ]),\n",
    "            tf.TensorShape([\n",
    "                None, None\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "    return result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./result_weights/tmp_weight/assets\n"
     ]
    }
   ],
   "source": [
    "gpt.save('./result_weights/tmp_weight', include_optimizer=False, signatures={\n",
    "    'serving_default': sample.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\"),\n",
    "        tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"length\")\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 837  259  497  788    8    9  411   71 9520   14    8   12    8    3\n",
      "     8]], shape=(1, 15), dtype=int64)\n",
      "今天天气 不错 , 不过 要 下雨 了 。 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气不错')\n",
    "\n",
    "ret = sample(\n",
    "    tf.constant([ids], dtype=tf.int64),\n",
    "    tf.constant(10, dtype=tf.int64)\n",
    ")\n",
    "print(ret)\n",
    "print(cbpe.decode(ret.numpy().tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read deploy model (you should restart kernel and go from here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from tf2gpt.model import GPT\n",
    "from utils.story_util import Story,Stories\n",
    "from utils.progress_bar import ProgressBar\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import random\n",
    "from utils.gpt2_tokenizer import GPT2Tokenizer\n",
    "import tensorflow_hub as hub\n",
    "cbpe = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = hub.load('./result_weights/tmp_weight/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gpt(tokenizer, gpt, sentence, number=1, length=20):\n",
    "    inputs = tf.constant([tokenizer.encode(sentence)] * number, dtype=tf.int64)\n",
    "    length = tf.constant(length, dtype=tf.int64)\n",
    "    ret = gpt.signatures['serving_default'](inp=inputs, length=length)['output_0']\n",
    "    return [\n",
    "        tokenizer.decode(s).replace(' ', '')\n",
    "        for s in ret.numpy()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.561 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个公司老总,你事业正在上升期,你娶了一个美丽的妻子\n",
      ">你走进你的家门\n",
      "你知道你即将成为一个人父,我有一个律师朋友,音乐家朋友,还有一个心理学家姐姐,他们都对自己孩子的成长充满了期待\n",
      ">我遇到的是怎么样一个律师事务所\n",
      "这是一个位于北京最中心的律师事务所,却聚集了中国律师界最精锐的力量\n",
      ">我遇到的是怎么样一个律师事务所\n",
      "这是一个位于北京最中心的律师事务所,却聚集了中国律师界最精锐的力量\n",
      ">我遇到的是怎么样一个律师事务所\n",
      "这是一个位于北京最中心的的的律师事务所,却聚集\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "q = f'''你是一个公司老总，你事业正在上升期，你娶了一个美丽的妻子\n",
    "> 你走进你的家门\n",
    "'''\n",
    "ret = sample_gpt(cbpe, gpt, q, 1, 150)\n",
    "for x in ret:\n",
    "    print(x)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Story():\n",
    "    def __init__(self,beginning,story_max_len=150,context_len=12):\n",
    "        self.story = [beginning]\n",
    "        self.story_max_len = story_max_len\n",
    "        self.context_len = context_len\n",
    "    \n",
    "    def action(self,action):\n",
    "        action_str = \"> 你\" + action\n",
    "        self.story.append(action_str)\n",
    "        q = \"\\n\".join(self.story[-self.context_len:]) + \"\\n\"\n",
    "        response = sample_gpt(cbpe, gpt, q, 1, 150)[0][len(q) - 1:]\n",
    "        \n",
    "        #responsesp = response.split(\"。\")\n",
    "        #if(len(responsesp) > 2):\n",
    "        #    response = ''.join(responsesp[:-1])\n",
    "        #else:\n",
    "        #    response = response.split(\">\")[0]\n",
    "        #response = \"。\".join([:-1])\n",
    "        response = response.split(\">\")[0].strip()\n",
    "        self.story.append(response)\n",
    "        \n",
    "    def interactive(self):\n",
    "        print(\"\\n\".join(self.story))\n",
    "        while True:\n",
    "            action = input(\"> 你\")\n",
    "            self.action(action)\n",
    "            print(self.story[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = f'''你在树林里冒险，指不定会从哪里蹦出来一些奇怪的东西，你握紧手上的手枪，希望这次冒险能够找到一些值钱的东西'''\n",
    "story = Story(begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你在树林里冒险，指不定会从哪里蹦出来一些奇怪的东西，你握紧手上的手枪，希望这次冒险能够找到一些值钱的东西\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 往树林深处走去\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你站直了身子,向树林深处走去。在他乡里,树林是这少数有人居住的区域,他们常见到这里黑暗而肃穆的气息。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 寻找人影\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "走到了尽头,朝四周扫了一眼,这里都是大片的未开发土地,除了种些菜的空地之外,便只有一些不小心闯入视线的树丛。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 走到一个村庄\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到镇子的主要街道上,从满身疲惫的士兵身上的军服,你依稀能分辨出前面的官道上,时常能看到巡逻的兵马和穿着制服的村民点头致意的士兵,这是一个男人应有的威严\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 和士兵攀谈\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前这个刚刚从战场上下来的男人,大胡子女人努力忍住想上前打断他们说话的冲动\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 继续向前走，找个酒馆\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这里有许多许小小的酒馆,因为你是这里唯一的英国人商人\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 进入最大的一个酒馆\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缘故,镇子里的铺子都变成了上了锁的空屋\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 点一杯啤酒\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国人,这里的酒吧拒绝和你攀谈的士兵和你交流\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 走出村子\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "歪斜砖墙和满是污垢的灯泡相互撞击的小酒吧窗格里漏进的黄光,酒香都早已飘到窗外来的青翠山茶树上的,一只灰色的小猴欢快跑过长满苔藓的石头走廊的声音,扑棱棱地从古董走廊里回来了彻夜写一个叫koon-ay的小说家,这个长相憨厚的中年男人,是一个狂热的登山爱好者\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> 你 问他在这里做什么\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "的做法事的男子,\n",
      "\n",
      "这是一个拥有火辣身材的土耳其裔男子,这里的旅行者用蹩脚的英语问来这座镇子的大城主人\n"
     ]
    }
   ],
   "source": [
    "story.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
